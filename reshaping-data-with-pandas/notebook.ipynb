{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Course Description**\n",
    "\n",
    "Often data is in a human-readable format, but it’s not suitable for data analysis. This is where pandas can help—it’s a powerful tool for reshaping DataFrames into different formats. In this course, you’ll grow your data scientist and analyst skills as you learn how to wrangle string columns and nested data contained in a DataFrame. You’ll work with real-world data, including FIFA player ratings, book reviews, and churn analysis data, as you learn how to reshape a DataFrame from wide to long format, stack and unstack rows and columns, and get descriptive statistics of a multi-index DataFrame.\n",
    "\n",
    "# Introduction to Data Reshaping\n",
    "\n",
    "Let's start by understanding the concept of wide and long formats and the advantages of using each of them. You’ll then learn how to pivot data from long to a wide format, and get summary statistics from a large DataFrame.\n",
    "\n",
    "## Wide and long data formats\n",
    "\n",
    "### The long and the wide\n",
    "\n",
    "As part of a data scientist job interview, you need to answer some technical questions. One of the challenges is to show the differences between long and wide data formats.\n",
    "\n",
    "In this exercise, you will classify a list of different characteristics, saying if they describe a DataFrame in wide format or a DataFrame in long format.\n",
    "\n",
    "-   Select the descriptions that are most appropriate for the different data formats: **wide format**, or **long format**.\n",
    "\n",
    "##### Long format\n",
    "\n",
    "- Each row represents one feature\n",
    "- There are multiple records for each observation\n",
    "- Needs a column to identify records of the same observation\n",
    "\n",
    "##### Wide format\n",
    "\n",
    "- Each feature is presented in a separate column\n",
    "- Each row shows many features of the same observation\n",
    "- Can contain large number of missing data\n",
    "- Does not contain repeated records\n",
    "\n",
    "### Flipping players\n",
    "\n",
    "Congratulations! You got the data scientist job! In your first project,\n",
    "you will work with the `fifa_players` dataset. It contains data of the\n",
    "players included in the last version of the video game. Before you start\n",
    "to do any analysis, you need to clean and format your dataset.\n",
    "\n",
    "As a first step, you need to explore your dataset and reshape it using\n",
    "basic steps, such as setting different indices, filtering columns and\n",
    "flipping the DataFrame. You would like to see if that is enough for\n",
    "further analysis.\n",
    "\n",
    "The `fifa_players` dataset is available for you. The `pandas` module\n",
    "will be preloaded as `pd` in your session throughout all the exercises\n",
    "of the course.\n",
    "\n",
    "-   Set the index of `fifa_players` to be the `name` column and assign\n",
    "    it to `fifa_transpose`.\n",
    "-   Modify the code to select only the columns `height` and `weight`\n",
    "    from the `fifa_players` DataFrame.\n",
    "-   Finally, transpose the `fifa_players` DataFrame so that the rows\n",
    "    become columns and the columns become rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   age  height  weight nationality                 club\n",
      "name                                                                   \n",
      "Lionel Messi        32     170      72   Argentina         FC Barcelona\n",
      "Cristiano Ronaldo   34     187      83    Portugal             Juventus\n",
      "Neymar da Silva     27     175      68      Brazil  Paris Saint-Germain\n",
      "Jan Oblak           26     188      87    Slovenia      Atlético Madrid\n",
      "Eden Hazard         28     175      74     Belgium          Real Madrid\n",
      "                   height  weight\n",
      "name                             \n",
      "Lionel Messi          170      72\n",
      "Cristiano Ronaldo     187      83\n",
      "Neymar da Silva       175      68\n",
      "Jan Oblak             188      87\n",
      "Eden Hazard           175      74\n",
      "name    Lionel Messi  Cristiano Ronaldo  Neymar da Silva  Jan Oblak   \n",
      "height           170                187              175        188  \\\n",
      "weight            72                 83               68         87   \n",
      "\n",
      "name    Eden Hazard  \n",
      "height          175  \n",
      "weight           74  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "import pandas as pd\n",
    "fifa_players = pd.read_csv('fifa_players.csv')\n",
    "\n",
    "# Set name as index\n",
    "fifa_transpose = fifa_players.set_index('name')\n",
    "\n",
    "# Print fifa_transpose\n",
    "print(fifa_transpose)\n",
    "\n",
    "# Filter the DataFrame to keep only height and weight columns\n",
    "fifa_transpose = fifa_players.set_index('name')[['height', 'weight']]\n",
    "\n",
    "# Print fifa_transpose\n",
    "print(fifa_transpose)\n",
    "\n",
    "# Change the DataFrame so rows become columns and vice versa\n",
    "fifa_transpose = fifa_players.set_index('name')[['height', 'weight']].transpose()\n",
    "\n",
    "# Print fifa_transpose\n",
    "print(fifa_transpose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping using pivot method\n",
    "\n",
    "### Dribbling the pivot method\n",
    "\n",
    "It's time to keep working with the `fifa_players` dataset. After you\n",
    "explored the dataset, you realized the dataset contains player scores on\n",
    "different movements: shooting, dribbling, and passing. There are\n",
    "attacking scores as well as overall scores.\n",
    "\n",
    "The goal of the project is to analyze the scores to create an optimized\n",
    "team, so you decide to explore which score is better. But the current\n",
    "data is in a long format. You'll need to to pivot your DataFrame in\n",
    "different ways to discover a pattern.\n",
    "\n",
    "The `fifa_players` dataset is available for you. *Make sure to examine\n",
    "it in the console*!\n",
    "\n",
    "Pivot `fifa_players` to get a DataFrame with `overall` scores indexed by\n",
    "`name`, and identified by `movement` in the columns.\n",
    "\n",
    "Pivot `fifa_players` to get a DataFrame with `attacking` scores indexed\n",
    "by `name`, and identified by `movement` in the columns.\n",
    "\n",
    "Use `.pivot()` on `fifa_players` to get `overall` scores indexed by\n",
    "`movement`, and identified by `name` in the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movement           dribbling  passing  shooting\n",
      "name                                           \n",
      "Cristiano Ronaldo         89       82        93\n",
      "L. Messi                  96       92        92\n",
      "movement           dribbling  passing  shooting\n",
      "name                                           \n",
      "Cristiano Ronaldo         84       83        89\n",
      "L. Messi                  88       92        70\n",
      "name       Cristiano Ronaldo  L. Messi\n",
      "movement                              \n",
      "dribbling                 89        96\n",
      "passing                   82        92\n",
      "shooting                  93        92\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "fifa_players = pd.read_csv('fifa_players_v1.csv')\n",
    "\n",
    "# Pivot fifa_players to get overall scores indexed by name and identified by movement\n",
    "fifa_overall = fifa_players.pivot(index='name', columns='movement', values='overall')\n",
    "\n",
    "# Print fifa_overall\n",
    "print(fifa_overall)\n",
    "\n",
    "# Pivot fifa_players to get attacking scores indexed by name and identified by movement\n",
    "fifa_attacking = fifa_players.pivot(index='name', columns='movement', values='attacking')\n",
    "\n",
    "# Print fifa_attacking\n",
    "print(fifa_attacking)\n",
    "\n",
    "# Use the pivot method to get overall scores indexed by movement and identified by name\n",
    "fifa_names = fifa_players.pivot(index='movement', columns='name', values='overall')\n",
    "\n",
    "# Print fifa_names\n",
    "print(fifa_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offensive or defensive player?\n",
    "\n",
    "You're not convinced with your previous exploration - you've discovered\n",
    "patterns in the `attacking` and `overall` scores in `fifa_players`. You\n",
    "would like to compare both scores, so you would like to see both in the\n",
    "same DataFrame.\n",
    "\n",
    "To do this, you'll need a way to pivot more than one column. You\n",
    "remember you can achieve this goal in two different ways: you could\n",
    "pivot the DataFrame using the list with the two columns, or you could\n",
    "extend the `.pivot()` method to all the columns present in the dataset.\n",
    "\n",
    "The `fifa_players` dataset is available for you. *Make sure to examine\n",
    "it in the console*!\n",
    "\n",
    "Pivot `fifa_players` to get `overall` and `attacking` scores indexed by\n",
    "`name`, and identified by `movement` in the columns.\n",
    "\n",
    "Use the `.pivot()` method on `fifa_players` to get all the scores\n",
    "indexed by `name`,and identified by `movement` in the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    overall                  attacking                 \n",
      "movement          dribbling passing shooting dribbling passing shooting\n",
      "name                                                                   \n",
      "Cristiano Ronaldo        89      82       93        84      83       89\n",
      "L. Messi                 96      92       92        88      92       70\n",
      "                    overall                  attacking                 \n",
      "movement          dribbling passing shooting dribbling passing shooting\n",
      "name                                                                   \n",
      "Cristiano Ronaldo        89      82       93        84      83       89\n",
      "L. Messi                 96      92       92        88      92       70\n"
     ]
    }
   ],
   "source": [
    "# Pivot fifa_players to get overall and attacking scores indexed by name and identified by movement\n",
    "fifa_over_attack = fifa_players.pivot(index='name', \n",
    "                                      columns='movement', \n",
    "                                      values=['overall', 'attacking'])\n",
    "\n",
    "# Print fifa_over_attack\n",
    "print(fifa_over_attack)\n",
    "\n",
    "# Use pivot method to get all the scores index by name and identified by movement\n",
    "fifa_all = fifa_players.pivot(index='name', columns='movement')\n",
    "\n",
    "# Print fifa_all\n",
    "print(fifa_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay that last move!\n",
    "\n",
    "Amazing! You were able to pivot all columns of `fifa_players`. You saw\n",
    "that the overall and attacking scores are different and decided to\n",
    "extend your analysis to more players. However, you found an error.\n",
    "\n",
    "You suspect that there are different scores for the same index and\n",
    "column values. You remember that using the `.pivot()` method for all the\n",
    "columns does not work in that case.\n",
    "\n",
    "First, you decide to delete the problematic row so you can reshape the\n",
    "DataFrame afterwards.\n",
    "\n",
    "The `fifa_players` dataset is available for you. *Make sure you examine\n",
    "the dataset into the console and notice the repeated rows*.\n",
    "\n",
    "-   Drop the fifth row of the `fifa_players` DataFrame.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Pivot `fifa_no_rep` to get all the scores indexed by `name`, and\n",
    "    identified by `movement` in the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           name   movement  overall  attacking\n",
      "typing.Literal[<no_default>]                                                  \n",
      "0                                      L. Messi   shooting       92         70\n",
      "1                             Cristiano Ronaldo   shooting       93         89\n",
      "2                                      L. Messi    passing       92         92\n",
      "3                             Cristiano Ronaldo    passing       82         83\n",
      "5                             Cristiano Ronaldo  dribbling       89         84\n",
      "                    overall                  attacking                 \n",
      "movement          dribbling passing shooting dribbling passing shooting\n",
      "name                                                                   \n",
      "Cristiano Ronaldo      89.0    82.0     93.0      84.0    83.0     89.0\n",
      "L. Messi                NaN    92.0     92.0       NaN    92.0     70.0\n"
     ]
    }
   ],
   "source": [
    "# Drop the fifth row to delete all repeated rows\n",
    "fifa_no_rep = fifa_players.drop(4, axis=0)\n",
    "\n",
    "# Print fifa_pivot\n",
    "print(fifa_no_rep)\n",
    "\n",
    "# Drop the fifth row to delete all repeated rows\n",
    "fifa_no_rep = fifa_players.drop(4, axis=0)\n",
    "\n",
    "# Pivot fifa players to get all scores by name and movement\n",
    "fifa_pivot = fifa_no_rep.pivot(index='name', columns='movement') \n",
    "\n",
    "# Print fifa_pivot\n",
    "print(fifa_pivot)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot tables\n",
    "\n",
    "### Reviewing the moves\n",
    "\n",
    "Wow! You have now learned about pivot tables. In the last analysis that\n",
    "you did, you encountered a DataFrame that had non-unique index/column\n",
    "pairs. In order to pivot your DataFrame, you wrote code to drop the last\n",
    "row, and then reshaped it.\n",
    "\n",
    "In this exercise, you will modify the code using pivot tables and\n",
    "compare it with your strategy of using the pivot method.\n",
    "\n",
    "The `fifa_players` dataset is available for you.\n",
    "\n",
    "-   Discard the fifth row of the `fifa_players` DataFrame.\n",
    "-   Use `.pivot()` on `fifa_players` to get all the scores indexed by\n",
    "    `name`, and identified by `movement` in the columns.\n",
    "-   Use a pivot table to show the mean of all scores by `name` and\n",
    "    `movement`, setting `name` as index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    overall                  attacking                 \n",
      "movement          dribbling passing shooting dribbling passing shooting\n",
      "name                                                                   \n",
      "Cristiano Ronaldo      89.0    82.0     93.0      84.0    83.0     89.0\n",
      "L. Messi                NaN    92.0     92.0       NaN    92.0     70.0\n",
      "                  attacking                    overall                 \n",
      "movement          dribbling passing shooting dribbling passing shooting\n",
      "name                                                                   \n",
      "Cristiano Ronaldo        84      83       89        89      82       93\n",
      "L. Messi                 88      92       70        96      92       92\n"
     ]
    }
   ],
   "source": [
    "# Discard the fifth row to delete all repeated rows\n",
    "fifa_drop = fifa_players.drop(4, axis=0)\n",
    "\n",
    "# Use pivot method to get all scores by name and movement\n",
    "fifa_pivot = fifa_drop.pivot(index='name', columns='movement') \n",
    "\n",
    "# Print fifa_pivot\n",
    "print(fifa_pivot)  \n",
    "\n",
    "# Use pivot table to get all scores by name and movement\n",
    "fifa_pivot_table = fifa_players.pivot_table(index='name', \n",
    "                                            columns='movement', \n",
    "                                            aggfunc='mean')\n",
    "# Print fifa_pivot_table\n",
    "print(fifa_pivot_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the big match\n",
    "\n",
    "Now, it's time to continue working on the `fifa_players` exploration.\n",
    "Your next task is to examine the characteristics of players belonging to\n",
    "different teams.\n",
    "\n",
    "Particularly, you are interested in players from two big rival teams:\n",
    "Barcelona and Real Madrid.\n",
    "\n",
    "You decide that `.pivot_table()` is the best tool to get your results\n",
    "since it's an easy way to generate a report. Also, it allows you to\n",
    "define aggregation functions and work with multiple indices.\n",
    "\n",
    "The `fifa_players` dataset is available for you. \\_Make sure you explore\n",
    "it. Check which data it contains from the players playing for each team.\n",
    "\n",
    "Use a pivot table to show the mean age of players in `fifa_players` by\n",
    "`club` and `nationality`. Set `nationality` as the index.\n",
    "\n",
    "Use a pivot table to show the maximum height of any player by `club` and\n",
    "`nationality`, setting `nationality` as the index.\n",
    "\n",
    "Define the DataFrame `players_country` that shows the player count by\n",
    "`club` and `nationality` and get the total count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "club         FC Barcelona  Real Madrid\n",
      "nationality                           \n",
      "Brazil          25.666667         23.5\n",
      "Croatia         31.000000         33.0\n",
      "France          23.600000         27.0\n",
      "Germany         27.000000         29.0\n",
      "Uruguay         32.000000         20.0\n",
      "club         FC Barcelona  Real Madrid\n",
      "nationality                           \n",
      "Brazil                190          186\n",
      "Croatia               184          172\n",
      "France                190          191\n",
      "Germany               187          183\n",
      "Uruguay               182          182\n",
      "club         FC Barcelona  Real Madrid  All\n",
      "nationality                                \n",
      "Brazil                  3            6    9\n",
      "Croatia                 1            1    2\n",
      "France                  5            3    8\n",
      "Germany                 1            1    2\n",
      "Uruguay                 1            1    2\n",
      "All                    11           12   23\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "fifa_players = pd.read_csv('fifa_players_v2.csv')\n",
    "\n",
    "# Use pivot table to display mean age of players by club and nationality\n",
    "mean_age_fifa = fifa_players.pivot_table(index='nationality', \n",
    "                                         columns='club', \n",
    "                                         values='age', \n",
    "                                         aggfunc='mean')\n",
    "\n",
    "# Print mean_age_fifa\n",
    "print(mean_age_fifa)\n",
    "\n",
    "# Use pivot table to display max height of any player by club and nationality\n",
    "tall_players_fifa = fifa_players.pivot_table(index='nationality', \n",
    "                                             columns='club', \n",
    "                                             values='height', \n",
    "                                             aggfunc='max')\n",
    "\n",
    "# Print tall_players_fifa\n",
    "print(tall_players_fifa)\n",
    "\n",
    "# Use pivot table to show the count of players by club and nationality and the total count\n",
    "players_country = fifa_players.pivot_table(index='nationality', \n",
    "                                           columns='club', \n",
    "                                           values='name', \n",
    "                                           aggfunc='count', \n",
    "                                           margins=True)\n",
    "\n",
    "# Print players_country\n",
    "print(players_country)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tallest and the heaviest\n",
    "\n",
    "You will continue your exploration of characteristics of players in\n",
    "`fifa_players` belonging to two teams: FC Barcelona and Real Madrid. As\n",
    "your last task, you are interested in exploring the maximum height and\n",
    "weight separated by teams and nationality. You will also compare two\n",
    "years, 2000 and 2010.\n",
    "\n",
    "You have two columns that you want to set as an index, so you will need\n",
    "to use `pivot_table()`.\n",
    "\n",
    "The `fifa_players` dataset is available for you. It contains data about\n",
    "the `club`, `nationality`, `height`, `weight`, and `year` of the players\n",
    "playing for each team.\n",
    "\n",
    "-   Use a pivot table to get all the values in the `year` column of the\n",
    "    `fifa_players` DataFrame, setting `nationality` and `club` as index.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Pass the appropriate function to `.pivot_table()` to show the\n",
    "    maximum values of the `year` columns.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Set the appropriate argument in `.pivot_table()` to get the maximum\n",
    "    for each row and column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         height      weight     \n",
      "year                       2000 2010   2000 2010\n",
      "nationality club                                \n",
      "Croatia     FC Barcelona    184  185     78   76\n",
      "            Real Madrid     172  173     66   68\n",
      "Germany     FC Barcelona    187  189     85   87\n",
      "            Real Madrid     183  185     76   77\n",
      "                         height      weight     \n",
      "year                       2000 2010   2000 2010\n",
      "nationality club                                \n",
      "Croatia     FC Barcelona    184  185     78   76\n",
      "            Real Madrid     172  173     66   68\n",
      "Germany     FC Barcelona    187  189     85   87\n",
      "            Real Madrid     183  185     76   77\n",
      "                         height           weight         \n",
      "year                       2000 2010  All   2000 2010 All\n",
      "nationality club                                         \n",
      "Croatia     FC Barcelona    184  185  185     78   76  78\n",
      "            Real Madrid     172  173  173     66   68  68\n",
      "Germany     FC Barcelona    187  189  189     85   87  87\n",
      "            Real Madrid     183  185  185     76   77  77\n",
      "All                         187  189  189     85   87  87\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "fifa_players = pd.read_csv('fifa_players_v3.csv')\n",
    "\n",
    "# Define a pivot table to get the characteristic by nationality and club\n",
    "fifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n",
    "                                     columns='year')\n",
    "\n",
    "# Print fifa_mean\n",
    "print(fifa_mean)\n",
    "\n",
    "# Set the appropriate argument to show the maximum values\n",
    "fifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n",
    "                                     columns='year', \n",
    "                                     aggfunc='max')\n",
    "\n",
    "# Print fifa_mean\n",
    "print(fifa_mean)\n",
    "\n",
    "# Set the argument to get the maximum for each row and column\n",
    "fifa_mean = fifa_players.pivot_table(index=['nationality', 'club'], \n",
    "                                     columns='year', \n",
    "                                     aggfunc='max', \n",
    "                                     margins=True)\n",
    "\n",
    "# Print fifa_mean\n",
    "print(fifa_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Between Wide and Long Format\n",
    "\n",
    "Master the technique of reshaping DataFrames from wide to long format. In this chapter, you'll learn how to use the melting method and wide to long function before discovering how to handle string columns by concatenating or splitting them.\n",
    "\n",
    "## Reshaping with melt\n",
    "\n",
    "### Gothic times\n",
    "\n",
    "You have a new project! Your boss has asked you to perform an analysis\n",
    "with a dataset of books. You have several tasks to complete. Your first\n",
    "task is to analyze Gothic fiction books.\n",
    "\n",
    "The dataset `books_gothic` is in a wide format. Any analysis will\n",
    "require you to reshape the data into a long format. To that aim, you\n",
    "will melt your dataset. You will reshape the dataset using several\n",
    "variables as identifiers to decide which is the best format.\n",
    "\n",
    "The `books_gothic` dataset is available for you. *Make sure to examine\n",
    "it in the console*!\n",
    "\n",
    "Define a `gothic_melted` DataFrame by melting the `books_gothic`\n",
    "DataFrame, using only the `title` as an identifier variable.\n",
    "\n",
    "Melt the `books_gothic` DataFrame, now using the `title`, `authors`, and\n",
    "`publisher` columns as identifier variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         title      variable              value\n",
      "0            Wuthering Heights       authors       Emily Bronte\n",
      "1                 Frankenstein       authors       Mary Shelley\n",
      "2   The Picture of Dorian Gray       authors        Oscar Wilde\n",
      "3            Wuthering Heights     num_pages                322\n",
      "4                 Frankenstein     num_pages                189\n",
      "5   The Picture of Dorian Gray     num_pages                187\n",
      "6            Wuthering Heights  rating_count               2155\n",
      "7                 Frankenstein  rating_count               2452\n",
      "8   The Picture of Dorian Gray  rating_count               3342\n",
      "9            Wuthering Heights        rating               3.85\n",
      "10                Frankenstein        rating               4.31\n",
      "11  The Picture of Dorian Gray        rating               4.15\n",
      "12           Wuthering Heights     publisher      Penguin Books\n",
      "13                Frankenstein     publisher  Kaplan Publishing\n",
      "14  The Picture of Dorian Gray     publisher            Pearson\n",
      "                        title       authors          publisher      variable   \n",
      "0           Wuthering Heights  Emily Bronte      Penguin Books     num_pages  \\\n",
      "1                Frankenstein  Mary Shelley  Kaplan Publishing     num_pages   \n",
      "2  The Picture of Dorian Gray   Oscar Wilde            Pearson     num_pages   \n",
      "3           Wuthering Heights  Emily Bronte      Penguin Books  rating_count   \n",
      "4                Frankenstein  Mary Shelley  Kaplan Publishing  rating_count   \n",
      "5  The Picture of Dorian Gray   Oscar Wilde            Pearson  rating_count   \n",
      "6           Wuthering Heights  Emily Bronte      Penguin Books        rating   \n",
      "7                Frankenstein  Mary Shelley  Kaplan Publishing        rating   \n",
      "8  The Picture of Dorian Gray   Oscar Wilde            Pearson        rating   \n",
      "\n",
      "     value  \n",
      "0   322.00  \n",
      "1   189.00  \n",
      "2   187.00  \n",
      "3  2155.00  \n",
      "4  2452.00  \n",
      "5  3342.00  \n",
      "6     3.85  \n",
      "7     4.31  \n",
      "8     4.15  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "books_gothic = pd.read_csv('books_gothic.csv')\n",
    "\n",
    "# Melt books_gothic using the title column as identifier\n",
    "gothic_melted = books_gothic.melt(id_vars='title')\n",
    "\n",
    "# Print gothic_melted\n",
    "print(gothic_melted)\n",
    "\n",
    "# Melt books_gothic using the title, authors, and publisher columns as identifier\n",
    "gothic_melted_new = books_gothic.melt(id_vars=['title', 'authors', 'publisher'])\n",
    "\n",
    "# Print gothic_melted_new\n",
    "print(gothic_melted_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating is not everything\n",
    "\n",
    "Your first exploration of the `books_gothic` dataset was successful.\n",
    "Now, your next task is to perform a more detailed analysis. You need to\n",
    "reshape your DataFrame again. This time, you don't want to use all of\n",
    "your variables.\n",
    "\n",
    "To that aim, you will melt your DataFrame, taking several approaches\n",
    "using different columns as identifiers and value variables.\n",
    "\n",
    "The same `books_gothic` dataset you used before is available for you. It\n",
    "contains data about the `title`, `author`, `number_pages`, `rating`,\n",
    "`rating_count`, and `publisher` of each book. *Make sure to examine it\n",
    "in the console*!\n",
    "\n",
    "Define a new DataFrame by melting the `publisher` column using the\n",
    "`title` and `authors` columns as identifier variables.\n",
    "\n",
    "Melt the `rating` and `rating_count` columns of `books_gothic` DataFrame\n",
    "using the `title` column as an identifier variable.\n",
    "\n",
    "Melt the `rating` and `rating_count` columns of `book_gothic` using the\n",
    "`title` and `authors` columns as identifier variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        title       authors   variable              value\n",
      "0           Wuthering Heights  Emily Bronte  publisher      Penguin Books\n",
      "1                Frankenstein  Mary Shelley  publisher  Kaplan Publishing\n",
      "2  The Picture of Dorian Gray   Oscar Wilde  publisher            Pearson\n",
      "                        title      variable    value\n",
      "0           Wuthering Heights        rating     3.85\n",
      "1                Frankenstein        rating     4.31\n",
      "2  The Picture of Dorian Gray        rating     4.15\n",
      "3           Wuthering Heights  rating_count  2155.00\n",
      "4                Frankenstein  rating_count  2452.00\n",
      "5  The Picture of Dorian Gray  rating_count  3342.00\n",
      "                        title       authors      variable    value\n",
      "0           Wuthering Heights  Emily Bronte        rating     3.85\n",
      "1                Frankenstein  Mary Shelley        rating     4.31\n",
      "2  The Picture of Dorian Gray   Oscar Wilde        rating     4.15\n",
      "3           Wuthering Heights  Emily Bronte  rating_count  2155.00\n",
      "4                Frankenstein  Mary Shelley  rating_count  2452.00\n",
      "5  The Picture of Dorian Gray   Oscar Wilde  rating_count  3342.00\n"
     ]
    }
   ],
   "source": [
    "# Melt publisher column using title and authors as identifiers\n",
    "publisher_melted = books_gothic.melt(id_vars=['title', 'authors'], \n",
    "                                     value_vars='publisher')\n",
    "\n",
    "# Print publisher_melted\n",
    "print(publisher_melted)\n",
    "\n",
    "# Melt rating and rating_count columns using the title as identifier\n",
    "rating_melted = books_gothic.melt(id_vars='title', \n",
    "                                  value_vars=['rating', 'rating_count'])\n",
    "\n",
    "# Print rating_melted\n",
    "print(rating_melted)\n",
    "\n",
    "# Melt rating and rating_count columns using title and authors as identifier\n",
    "books_melted = books_gothic.melt(id_vars=['title', 'authors'], \n",
    "                                 value_vars=['rating', 'rating_count'])\n",
    "\n",
    "# Print books_melted\n",
    "print(books_melted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is Frankenstein, Dorian Gray?\n",
    "\n",
    "You are satisfied with the way you reshaped the `books_gothic`\n",
    "DataFrame, however, you would like to finish your work by naming the\n",
    "newly-created columns. This will help you clarify what the variables and\n",
    "values are.\n",
    "\n",
    "You remember that `.melt()` allows you to do that. In order to achieve\n",
    "your goal, you will reshape your DataFrame in three steps.\n",
    "\n",
    "The same `books_gothic` dataset you used before is available for you. It\n",
    "contains data about the `title`, `author`, `number_pages`, `rating`,\n",
    "`rating_count`, and `publisher` of each book. *Make sure to examine it\n",
    "in the console*!\n",
    "\n",
    "-   Define a new `books_ratings` DataFrame by melting the `rating` and\n",
    "    `rating_count` columns using the `title`, `authors`, and `publisher`\n",
    "    as identifier variables.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Inside the `.melt()` call, assign the name `'feature'` to the column\n",
    "    that contains the variable names.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Inside the `.melt()` call, assign the name `'number'` to the\n",
    "    resulting value column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        title       authors          publisher      variable   \n",
      "0           Wuthering Heights  Emily Bronte      Penguin Books        rating  \\\n",
      "1                Frankenstein  Mary Shelley  Kaplan Publishing        rating   \n",
      "2  The Picture of Dorian Gray   Oscar Wilde            Pearson        rating   \n",
      "3           Wuthering Heights  Emily Bronte      Penguin Books  rating_count   \n",
      "4                Frankenstein  Mary Shelley  Kaplan Publishing  rating_count   \n",
      "5  The Picture of Dorian Gray   Oscar Wilde            Pearson  rating_count   \n",
      "\n",
      "     value  \n",
      "0     3.85  \n",
      "1     4.31  \n",
      "2     4.15  \n",
      "3  2155.00  \n",
      "4  2452.00  \n",
      "5  3342.00  \n",
      "                        title       authors          publisher       feature   \n",
      "0           Wuthering Heights  Emily Bronte      Penguin Books        rating  \\\n",
      "1                Frankenstein  Mary Shelley  Kaplan Publishing        rating   \n",
      "2  The Picture of Dorian Gray   Oscar Wilde            Pearson        rating   \n",
      "3           Wuthering Heights  Emily Bronte      Penguin Books  rating_count   \n",
      "4                Frankenstein  Mary Shelley  Kaplan Publishing  rating_count   \n",
      "5  The Picture of Dorian Gray   Oscar Wilde            Pearson  rating_count   \n",
      "\n",
      "     value  \n",
      "0     3.85  \n",
      "1     4.31  \n",
      "2     4.15  \n",
      "3  2155.00  \n",
      "4  2452.00  \n",
      "5  3342.00  \n",
      "                        title       authors          publisher       feature   \n",
      "0           Wuthering Heights  Emily Bronte      Penguin Books        rating  \\\n",
      "1                Frankenstein  Mary Shelley  Kaplan Publishing        rating   \n",
      "2  The Picture of Dorian Gray   Oscar Wilde            Pearson        rating   \n",
      "3           Wuthering Heights  Emily Bronte      Penguin Books  rating_count   \n",
      "4                Frankenstein  Mary Shelley  Kaplan Publishing  rating_count   \n",
      "5  The Picture of Dorian Gray   Oscar Wilde            Pearson  rating_count   \n",
      "\n",
      "    number  \n",
      "0     3.85  \n",
      "1     4.31  \n",
      "2     4.15  \n",
      "3  2155.00  \n",
      "4  2452.00  \n",
      "5  3342.00  \n"
     ]
    }
   ],
   "source": [
    "# Melt the rating and rating_count using title, authors and publisher as identifiers\n",
    "books_ratings = books_gothic.melt(id_vars=['title', 'authors', 'publisher'], \n",
    "                                  value_vars=['rating', 'rating_count'])\n",
    "\n",
    "# Print books_ratings\n",
    "print(books_ratings)\n",
    "\n",
    "# Assign the name feature to the new variable column\n",
    "books_ratings = books_gothic.melt(id_vars=['title', 'authors', 'publisher'], \n",
    "                                  value_vars=['rating', 'rating_count'], \n",
    "                                  var_name='feature')\n",
    "\n",
    "# Print books_ratings\n",
    "print(books_ratings)\n",
    "\n",
    "# Assign the name number to the new column containing the values\n",
    "books_ratings = books_gothic.melt(id_vars=['title', 'authors', 'publisher'], \n",
    "                                  value_vars=['rating', 'rating_count'], \n",
    "                                  var_name='feature', \n",
    "                                  value_name='number')\n",
    "\n",
    "# Print books_ratings\n",
    "print(books_ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wide to long function\n",
    "\n",
    "### The golden age\n",
    "\n",
    "In this exercise, you'll continue working on the book project. Now,\n",
    "you'll analyze books from the Golden Age.\n",
    "\n",
    "Upon inspection, you discovered that the dataset `golden_age` needs\n",
    "reshaping. You noticed that some column names start with the same prefix\n",
    "(stub names) and identified other columns to use as unique IDs.\n",
    "\n",
    "For that reason, you'll reshape your DataFrame in several ways. Use the\n",
    "function `wide_to_long()` along with each instruction.\n",
    "\n",
    "The `golden_age` dataset is available for you. It contains the `title`,\n",
    "`authors`, and data about the universal identifier ISBN and prefix for\n",
    "countries of each book.\n",
    "\n",
    "Set `title` as the unique index. Extract the prefix from `isbn10` and\n",
    "`isbn13`. Name the new variable created `version`.\n",
    "\n",
    "Set `title` and `authors` as the index. Get the prefix from `prefix10`\n",
    "and `prefix13`. Name the new variable `version`.\n",
    "\n",
    "Set `title` and `authors` as unique indexes. Extract the prefixes `isbn`\n",
    "and `prefix`. Name the new variable `version`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           prefix13              authors  prefix10   \n",
      "title             version                                            \n",
      "The Great Gatsby  13            978  F. Scott Fitzgerald         1  \\\n",
      "The Short Stories 13            978     Ernest Hemingway         0   \n",
      "To the Lighthouse 13            978       Virginia Woolf         0   \n",
      "The Great Gatsby  10            978  F. Scott Fitzgerald         1   \n",
      "The Short Stories 10            978     Ernest Hemingway         0   \n",
      "To the Lighthouse 10            978       Virginia Woolf         0   \n",
      "\n",
      "                                    isbn  \n",
      "title             version                 \n",
      "The Great Gatsby  13       9780060098919  \n",
      "The Short Stories 13       9780684837864  \n",
      "To the Lighthouse 13       9780156030472  \n",
      "The Great Gatsby  10          1572702567  \n",
      "The Short Stories 10           684837862  \n",
      "To the Lighthouse 10           156030470  \n",
      "                                                      isbn13      isbn10   \n",
      "title             authors             version                              \n",
      "The Great Gatsby  F. Scott Fitzgerald 13       9780060098919  1572702567  \\\n",
      "                                      10       9780060098919  1572702567   \n",
      "The Short Stories Ernest Hemingway    13       9780684837864   684837862   \n",
      "                                      10       9780684837864   684837862   \n",
      "To the Lighthouse Virginia Woolf      13       9780156030472   156030470   \n",
      "                                      10       9780156030472   156030470   \n",
      "\n",
      "                                               prefix  \n",
      "title             authors             version          \n",
      "The Great Gatsby  F. Scott Fitzgerald 13          978  \n",
      "                                      10            1  \n",
      "The Short Stories Ernest Hemingway    13          978  \n",
      "                                      10            0  \n",
      "To the Lighthouse Virginia Woolf      13          978  \n",
      "                                      10            0  \n",
      "                                                        isbn  prefix\n",
      "title             authors             version                       \n",
      "The Great Gatsby  F. Scott Fitzgerald 13       9780060098919     978\n",
      "                                      10          1572702567       1\n",
      "The Short Stories Ernest Hemingway    13       9780684837864     978\n",
      "                                      10           684837862       0\n",
      "To the Lighthouse Virginia Woolf      13       9780156030472     978\n",
      "                                      10           156030470       0\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "golden_age = pd.read_csv('golden_age.csv')\n",
    "\n",
    "# Reshape wide to long using title as index and version as new name, and extracting isbn prefix \n",
    "isbn_long = pd.wide_to_long(golden_age, \n",
    "                            stubnames='isbn', \n",
    "                            i='title', \n",
    "                            j='version')\n",
    "\n",
    "# Print isbn_long\n",
    "print(isbn_long)\n",
    "\n",
    "# Reshape wide to long using title and authors as index and version as new name, and prefix as stubnames \n",
    "prefix_long = pd.wide_to_long(golden_age, \n",
    "                              stubnames='prefix', \n",
    "                              i=['title', 'authors'], \n",
    "                              j='version')\n",
    "\n",
    "# Print prefix_long\n",
    "print(prefix_long)\n",
    "\n",
    "# Reshape wide to long using title and authors as index and version as new name, and prefix and isbn as wide column prefixes \n",
    "all_long = pd.wide_to_long(golden_age, \n",
    "                           stubnames=['isbn', 'prefix'], \n",
    "                           i=['title', 'authors'], \n",
    "                           j='version')\n",
    "\n",
    "# Print all_long\n",
    "print(all_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decrypting the code\n",
    "\n",
    "You are doing a great job on the book project! Your boss encouraged you\n",
    "to do an analysis using books written by Dan Brown.\n",
    "\n",
    "You explored the dataset `books_brown` and it needs reshaping. Again,\n",
    "you identified several columns to use as unique IDs and realized\n",
    "something different about the columns to group. Their name starts with a\n",
    "prefix, but this time, you identified a suffix and a separation element.\n",
    "\n",
    "The `books_brown` dataset is available for you. It contains the `title`,\n",
    "`author`, and data about `language_code`, `language_name`,\n",
    "`publisher_code`, and `publisher_name` of each book. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "-   Reshape `books_brown` from wide to long format, using the columns\n",
    "    `author` and `title` as unique indexes. Name `'code'` the new column\n",
    "    created from the columns starting with `language` and `publisher`.\n",
    "    Don't forget to examine the printed output.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Inside the `wide_to_long()` call, specify an underscore as the\n",
    "    separator between the variable names in the wide columns. Don't\n",
    "    forget to examine the printed output.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Inside the `wide_to_long()` call, specify that the wide column names\n",
    "    end in a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [publisher_name, language_code, publisher_code, language_name, language, publisher]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [publisher_name, language_code, publisher_code, language_name, language, publisher]\n",
      "Index: []\n",
      "                                    language     publisher\n",
      "author    title                code                       \n",
      "Dan Brown The Da Vinci Code    code        0            12\n",
      "                               name  english  Random House\n",
      "          Angels & Demons      code        0            34\n",
      "                               name  english  Pocket Books\n",
      "          La fortaleza digital code       84            43\n",
      "                               name  spanish       Umbriel\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "books_brown = pd.read_csv('books_brown.csv')\n",
    "\n",
    "# Reshape using author and title as index, code as new name and getting the prefix language and publisher\n",
    "the_code_long = pd.wide_to_long(books_brown, \n",
    "                                stubnames=['language', 'publisher'], \n",
    "                                i=['author', 'title'], \n",
    "                                j='code')\n",
    "\n",
    "# Print the_code_long\n",
    "print(the_code_long)\n",
    "\n",
    "# Specify underscore as the character that separates the variable names\n",
    "the_code_long = pd.wide_to_long(books_brown, \n",
    "                                stubnames=['language', 'publisher'], \n",
    "                                i=['author', 'title'], \n",
    "                                j='code', \n",
    "                                sep='_')\n",
    "\n",
    "# Print the_code_long\n",
    "print(the_code_long)\n",
    "\n",
    "# Specify that wide columns have a suffix containing words\n",
    "the_code_long = pd.wide_to_long(books_brown, \n",
    "                                stubnames=['language', 'publisher'], \n",
    "                                i=['author', 'title'], \n",
    "                                j='code', \n",
    "                                sep='_', \n",
    "                                suffix='\\w+')\n",
    "\n",
    "# Print the_code_long\n",
    "print(the_code_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to read, Katniss!\n",
    "\n",
    "It's almost time to finish working for the day. But first, you would\n",
    "like to do an analysis for fun. You will analyze another book dataset,\n",
    "this time with the *Hunger Games* series.\n",
    "\n",
    "You explored the dataset `books_hunger` before reshaping it, but\n",
    "something was not right. The index of the DataFrame contains the title\n",
    "of the books. You know that you cannot reshape it in this format. If you\n",
    "do, you will lose valuable data, the title, so you'll need to make some\n",
    "changes before transforming the DataFrame.\n",
    "\n",
    "The `books_hunger` dataset is available for you. It contains the\n",
    "`title`, and data about `language`, `publication date`,\n",
    "`publication number`, and `page number` of each book.\n",
    "\n",
    "-   Modify the `books_hunger` DataFrame by resetting the index without\n",
    "    dropping it.\n",
    "-   Reshape `books_hunger` from wide to long format. Use the columns\n",
    "    `title` and `language` as unique indexes. Name `feature` the new\n",
    "    variable created from the columns that starts with `publication` and\n",
    "    `page`. Those columns are separated by a blank space and end in a\n",
    "    word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        publication   page\n",
      "title                  language feature                   \n",
      "Los Juegos del Hambre  Spanish  date      5/25/2010    NaN\n",
      "                                number            2  374.0\n",
      "Catching Fire          English  date      5/25/2012    NaN\n",
      "                                number            6  391.0\n",
      "Il canto della rivolta Italian  date       6/8/2015    NaN\n",
      "                                number            4  390.0\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "books_hunger = pd.read_csv('books_hunger.csv', index_col = 'title')\n",
    "\n",
    "# Modify books_hunger by resetting the index without dropping it\n",
    "books_hunger.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Reshape using title and language as index, feature as new name, publication and page as prefix separated by space and ending in a word\n",
    "publication_features = pd.wide_to_long(books_hunger, \n",
    "                                       stubnames=['publication', 'page'], \n",
    "                                       i=['title', 'language'], \n",
    "                                       j='feature', \n",
    "                                       sep=' ', \n",
    "                                       suffix='\\w+')\n",
    "\n",
    "# Print publication_features\n",
    "print(publication_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with string columns\n",
    "\n",
    "### Did you say dystopia?\n",
    "\n",
    "Another day at work, another day working with your book project! You are\n",
    "very excited because you have been making a lot of progress. You plan to\n",
    "work on a dataset about dystopian fiction books.\n",
    "\n",
    "But first, you need to do some string manipulations. You realize that\n",
    "the DataFrame index contains data about the title and the release year.\n",
    "You can't find a column with the author of the book, so you decide to\n",
    "pre-define a list of the writers. Then, you want to delete the year and\n",
    "replace it with the author.\n",
    "\n",
    "You decide that splitting the index and then concatenating it with the\n",
    "list is the best way to do it.\n",
    "\n",
    "The `books_dys` dataset and `author_list` are available for you. *Make\n",
    "sure to examine it in the console*!\n",
    "\n",
    "-   Split the string separated by a hyphen contained in the index of\n",
    "    `books_dys`. Assign it to the index.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Now that you've split the index, get the first element and assign it\n",
    "    to the index of `books_dys`.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Concatenate the current index of `books_dys` with the pre-defined\n",
    "    `author_list`, using a hyphen as a separating element. Assign it to\n",
    "    the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         year  num_pages  average_rating  ratings_count\n",
      "title                                                                  \n",
      "[Fahrenheit 451, 1953]   1953        186            4.10          23244\n",
      "[1984, 1949]             1949        268            4.31          14353\n",
      "[Brave New World, 1932]  1932        123            4.30          23535\n",
      "                 year  num_pages  average_rating  ratings_count\n",
      "title                                                          \n",
      "Fahrenheit 451   1953        186            4.10          23244\n",
      "1984             1949        268            4.31          14353\n",
      "Brave New World  1932        123            4.30          23535\n",
      "                               year  num_pages  average_rating  ratings_count\n",
      "title                                                                        \n",
      "Fahrenheit 451-Ray Bradbury    1953        186            4.10          23244\n",
      "1984-George Orwell             1949        268            4.31          14353\n",
      "Brave New World-Aldous Huxley  1932        123            4.30          23535\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "books_dys = pd.read_csv('books_dys.csv', index_col = 'title')\n",
    "\n",
    "# Split the index of books_dys by the hyphen \n",
    "books_dys.index = books_dys.index.str.split('-')\n",
    "\n",
    "# Print books_dys\n",
    "print(books_dys)\n",
    "\n",
    "# edited/added\n",
    "books_dys = pd.read_csv('books_dys.csv', index_col = 'title')\n",
    "\n",
    "# Get the first element after splitting the index of books_dys\n",
    "books_dys.index = books_dys.index.str.split('-').str.get(0)\n",
    "\n",
    "# Print books_dys\n",
    "print(books_dys)\n",
    "\n",
    "# edited/added\n",
    "books_dys = pd.read_csv('books_dys.csv', index_col = 'title')\n",
    "author_list = ['Ray Bradbury', 'George Orwell', 'Aldous Huxley']\n",
    "\n",
    "# Split by the hyphen the index of books_dys\n",
    "books_dys.index = books_dys.index.str.split('-').str.get(0)\n",
    "\n",
    "# Concatenate the index with the list author_list separated by a hyphen\n",
    "books_dys.index = books_dys.index.str.cat(author_list, sep='-')\n",
    "\n",
    "# Print books_dys\n",
    "print(books_dys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's your rating, Harry?\n",
    "\n",
    "You fix yourself a coffee and keep working on your book project. For\n",
    "your next task, you need to get an appropriate dataset containing\n",
    "ratings for all the Harry Potter books. You gathered data from Goodreads\n",
    "as well as from Amazon.\n",
    "\n",
    "You realized that you need a long format, but the dataset `hp_books` is\n",
    "in a wide format. You want to melt the data, but first, you need to\n",
    "manipulate some of the string columns.\n",
    "\n",
    "The full title is divided into two columns. The `authors` column\n",
    "contains info about the writer and the illustrator.\n",
    "\n",
    "Ratings for the Harry Potter books are in the DataFrame `hp_books`.\n",
    "*Make sure to examine it in the console*!\n",
    "\n",
    "-   Concatenate the `title` and `subtitle` columns into a column named\n",
    "    `full_title`. Use the word `'and'` separated by spaces as a\n",
    "    separating element.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Split the `authors` column into two columns called `writer` and\n",
    "    `illustrator`. Use the slash character `/` as the delimiter.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Define a DataFrame `hp_melt` by melting the `goodreads` and `amazon`\n",
    "    columns into a single column named `source`. Assign the name\n",
    "    `rating` to the resulting value column. Use only the full title and\n",
    "    the writer as identifier variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          title                  subtitle                     authors   \n",
      "0  Harry Potter     the Half-Blood Prince  J.K. Rowling/Mary GrandPré  \\\n",
      "1  Harry Potter  the Order of the Phoenix  J.K. Rowling/Mary GrandPré   \n",
      "2  Harry Potter    the Chamber of Secrets                J.K. Rowling   \n",
      "3  Harry Potter   the Prisoner of Azkaban  J.K. Rowling/Mary GrandPré   \n",
      "4  Harry Potter       The Deathly Hallows  J.K. Rowling/Mary GrandPré   \n",
      "5  Harry Potter      the Sorcerer's Stone  J.K. Rowling/Mary GrandPré   \n",
      "6  Harry Potter        the Goblet of Fire                J.K. Rowling   \n",
      "\n",
      "   goodreads  amazon                                 full_title  \n",
      "0       4.57    4.52     Harry Potter and the Half-Blood Prince  \n",
      "1       4.49    4.44  Harry Potter and the Order of the Phoenix  \n",
      "2       4.42    4.37    Harry Potter and the Chamber of Secrets  \n",
      "3       4.56    4.51   Harry Potter and the Prisoner of Azkaban  \n",
      "4       4.42    4.37       Harry Potter and The Deathly Hallows  \n",
      "5       4.47    4.42      Harry Potter and the Sorcerer's Stone  \n",
      "6       4.56    4.51        Harry Potter and the Goblet of Fire  \n",
      "          title                  subtitle                     authors   \n",
      "0  Harry Potter     the Half-Blood Prince  J.K. Rowling/Mary GrandPré  \\\n",
      "1  Harry Potter  the Order of the Phoenix  J.K. Rowling/Mary GrandPré   \n",
      "2  Harry Potter    the Chamber of Secrets                J.K. Rowling   \n",
      "3  Harry Potter   the Prisoner of Azkaban  J.K. Rowling/Mary GrandPré   \n",
      "4  Harry Potter       The Deathly Hallows  J.K. Rowling/Mary GrandPré   \n",
      "5  Harry Potter      the Sorcerer's Stone  J.K. Rowling/Mary GrandPré   \n",
      "6  Harry Potter        the Goblet of Fire                J.K. Rowling   \n",
      "\n",
      "   goodreads  amazon                                 full_title        writer   \n",
      "0       4.57    4.52     Harry Potter and the Half-Blood Prince  J.K. Rowling  \\\n",
      "1       4.49    4.44  Harry Potter and the Order of the Phoenix  J.K. Rowling   \n",
      "2       4.42    4.37    Harry Potter and the Chamber of Secrets  J.K. Rowling   \n",
      "3       4.56    4.51   Harry Potter and the Prisoner of Azkaban  J.K. Rowling   \n",
      "4       4.42    4.37       Harry Potter and The Deathly Hallows  J.K. Rowling   \n",
      "5       4.47    4.42      Harry Potter and the Sorcerer's Stone  J.K. Rowling   \n",
      "6       4.56    4.51        Harry Potter and the Goblet of Fire  J.K. Rowling   \n",
      "\n",
      "     illustrator  \n",
      "0  Mary GrandPré  \n",
      "1  Mary GrandPré  \n",
      "2           None  \n",
      "3  Mary GrandPré  \n",
      "4  Mary GrandPré  \n",
      "5  Mary GrandPré  \n",
      "6           None  \n",
      "                                   full_title        writer     source  rating\n",
      "0      Harry Potter and the Half-Blood Prince  J.K. Rowling  goodreads    4.57\n",
      "1   Harry Potter and the Order of the Phoenix  J.K. Rowling  goodreads    4.49\n",
      "2     Harry Potter and the Chamber of Secrets  J.K. Rowling  goodreads    4.42\n",
      "3    Harry Potter and the Prisoner of Azkaban  J.K. Rowling  goodreads    4.56\n",
      "4        Harry Potter and The Deathly Hallows  J.K. Rowling  goodreads    4.42\n",
      "5       Harry Potter and the Sorcerer's Stone  J.K. Rowling  goodreads    4.47\n",
      "6         Harry Potter and the Goblet of Fire  J.K. Rowling  goodreads    4.56\n",
      "7      Harry Potter and the Half-Blood Prince  J.K. Rowling     amazon    4.52\n",
      "8   Harry Potter and the Order of the Phoenix  J.K. Rowling     amazon    4.44\n",
      "9     Harry Potter and the Chamber of Secrets  J.K. Rowling     amazon    4.37\n",
      "10   Harry Potter and the Prisoner of Azkaban  J.K. Rowling     amazon    4.51\n",
      "11       Harry Potter and The Deathly Hallows  J.K. Rowling     amazon    4.37\n",
      "12      Harry Potter and the Sorcerer's Stone  J.K. Rowling     amazon    4.42\n",
      "13        Harry Potter and the Goblet of Fire  J.K. Rowling     amazon    4.51\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "hp_books = pd.read_csv('hp_books.csv')\n",
    "\n",
    "# Concatenate the title and subtitle separated by \"and\" surrounded by spaces\n",
    "hp_books['full_title'] = hp_books['title'].str.cat(hp_books['subtitle'], sep =\" and \") \n",
    "\n",
    "# Print hp_books\n",
    "print(hp_books)\n",
    "\n",
    "# Concatenate the title and subtitle separated by \"and\" surrounded by spaces\n",
    "hp_books['full_title'] = hp_books['title'].str.cat(hp_books['subtitle'], sep =\" and \") \n",
    "\n",
    "# Split the authors into writer and illustrator columns\n",
    "hp_books[['writer', 'illustrator']] = hp_books['authors'].str.split('/', expand=True) \n",
    "\n",
    "# Print hp_books\n",
    "print(hp_books)\n",
    "\n",
    "# Concatenate the title and subtitle separated by \"and\" surrounded by spaces\n",
    "hp_books['full_title'] = hp_books['title'].str.cat(hp_books['subtitle'], sep =\" and \") \n",
    "\n",
    "# Split the authors into writer and illustrator columns\n",
    "hp_books[['writer', 'illustrator']] = hp_books['authors'].str.split('/', expand=True)\n",
    "\n",
    "# Melt goodreads and amazon columns into a single column \n",
    "hp_melt = hp_books.melt(id_vars=['full_title', 'writer'], \n",
    "                        var_name='source', \n",
    "                        value_vars=['goodreads', 'amazon'], \n",
    "                        value_name='rating')\n",
    "\n",
    "# Print hp_melt\n",
    "print(hp_melt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementary, dear Watson!\n",
    "\n",
    "It's Friday, and you are about to finish working on your book project.\n",
    "For your last task, you will analyze data about Arthur Conan Doyle's\n",
    "books.\n",
    "\n",
    "You realize your dataset, `books_sh`, needs reshaping. You notice there\n",
    "are columns that can be grouped using a prefix. You identify the columns\n",
    "to use as unique IDs. However, some of these columns contain strings.\n",
    "They need some manipulation before applying a wide to long\n",
    "transformation. You decide some of the strings need splitting to make\n",
    "the DataFrame cleaner.\n",
    "\n",
    "The `books_sh` dataset is available for you. It contains the *title*,\n",
    "and data about `version`, `number_pages`, and `number_ratings` of each\n",
    "book.\n",
    "\n",
    "-   Split the `main_title` column into two columns called `title` and\n",
    "    `subtitle`. Use a colon as the delimiter.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Split the string separated by a blank space contained in the\n",
    "    `version` column. Assign the second element to a new column called\n",
    "    `volume`.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Drop the columns named `main_title` and `version`, modifying the\n",
    "    `books_sh` DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               main_title version  number_pages   \n",
      "0    Sherlock Holmes: The Complete Novels   Vol I          1059  \\\n",
      "1    Sherlock Holmes: The Complete Novels  Vol II           709   \n",
      "2  Adventures of Sherlock Holmes: Memoirs   Vol I           334   \n",
      "3  Adventures of Sherlock Holmes: Memoirs  Vol II           238   \n",
      "\n",
      "   number_ratings                          title              subtitle  \n",
      "0           24087                Sherlock Holmes   The Complete Novels  \n",
      "1           26794                Sherlock Holmes   The Complete Novels  \n",
      "2            2184  Adventures of Sherlock Holmes               Memoirs  \n",
      "3            1884  Adventures of Sherlock Holmes               Memoirs  \n",
      "                               main_title version  number_pages   \n",
      "0    Sherlock Holmes: The Complete Novels   Vol I          1059  \\\n",
      "1    Sherlock Holmes: The Complete Novels  Vol II           709   \n",
      "2  Adventures of Sherlock Holmes: Memoirs   Vol I           334   \n",
      "3  Adventures of Sherlock Holmes: Memoirs  Vol II           238   \n",
      "\n",
      "   number_ratings                          title              subtitle volume  \n",
      "0           24087                Sherlock Holmes   The Complete Novels      I  \n",
      "1           26794                Sherlock Holmes   The Complete Novels     II  \n",
      "2            2184  Adventures of Sherlock Holmes               Memoirs      I  \n",
      "3            1884  Adventures of Sherlock Holmes               Memoirs     II  \n",
      "   number_pages  number_ratings                          title   \n",
      "0          1059           24087                Sherlock Holmes  \\\n",
      "1           709           26794                Sherlock Holmes   \n",
      "2           334            2184  Adventures of Sherlock Holmes   \n",
      "3           238            1884  Adventures of Sherlock Holmes   \n",
      "\n",
      "               subtitle volume  \n",
      "0   The Complete Novels      I  \n",
      "1   The Complete Novels     II  \n",
      "2               Memoirs      I  \n",
      "3               Memoirs     II  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "books_sh = pd.read_csv('books_sh.csv')\n",
    "\n",
    "# Split main_title by a colon and assign it to two columns named title and subtitle \n",
    "books_sh[['title', 'subtitle']] = books_sh['main_title'].str.split(':', expand=True)\n",
    "\n",
    "# Print books_sh\n",
    "print(books_sh)\n",
    "\n",
    "# Split main_title by a colon and assign it to two columns named title and subtitle \n",
    "books_sh[['title', 'subtitle']] = books_sh['main_title'].str.split(':', expand=True)\n",
    "\n",
    "# Split version by a space and assign the second element to the column named volume\n",
    "books_sh['volume'] = books_sh['version'].str.split(' ').str.get(1)\n",
    "\n",
    "# Print books_sh\n",
    "print(books_sh)\n",
    "\n",
    "# Split main_title by a colon and assign it to two columns named title and subtitle \n",
    "books_sh[['title', 'subtitle']] = books_sh['main_title'].str.split(':', expand=True)\n",
    "\n",
    "# Split version by a space and assign the second element to the column named volume\n",
    "books_sh['volume'] = books_sh['version'].str.split(' ').str.get(1)\n",
    "\n",
    "# Drop the main_title and version columns modifying books_sh\n",
    "books_sh.drop(['main_title', 'version'], axis=1, inplace=True)\n",
    "\n",
    "# Print books_sh\n",
    "print(books_sh)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking and Unstacking DataFrames\n",
    "\n",
    "In this chapter, you’ll level-up your data manipulation skills using multi-level indexing. You'll learn how to reshape DataFrames by rearranging levels of the row indexes to the column axis, or vice versa. You'll also gain the skills you need to handle missing data generated in the stacking and unstacking processes.\n",
    "\n",
    "## Stacking DataFrames\n",
    "\n",
    "### Stack the calls!\n",
    "\n",
    "New week, new project! One of your clients, a telecommunication company,\n",
    "wants to know why its customers are leaving. You will perform an\n",
    "analysis to figure it out. First, you explored the dataset `churn` and\n",
    "realized some information is missing. The dataset contains data about\n",
    "the total number of calls and the minutes spent on the phone by\n",
    "different customers. However, the state and city they live in are not\n",
    "listed.\n",
    "\n",
    "You predefined an array with that data. You'd like to add it as an index\n",
    "in your DataFrame.\n",
    "\n",
    "The DataFrame `churn` is available for you. It contains data about\n",
    "`area code`, `total_day_calls` and `total_day_minutes`. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "-   Create a new multi-level index using the `new_index` list and the\n",
    "    appropriate `pandas` method. Name the levels `state` and `city`\n",
    "    respectively.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Assign the multi-level index contained in `churn_new` as the index\n",
    "    of the `churn` DataFrame.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Reshape the `churn` DataFrame by stacking and assign it to\n",
    "    `churn_stack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiIndex([('California',   'Los Angeles'),\n",
      "            ('California', 'San Francisco'),\n",
      "            (  'New York',      'New York'),\n",
      "            (      'Ohio',     'Cleveland')],\n",
      "           names=['state', 'city'])\n",
      "                          Area code  total_day_calls  total_day_minutes\n",
      "state      city                                                        \n",
      "California Los Angeles          408              116                204\n",
      "           San Francisco        408              109                287\n",
      "New York   New York             415               84                 84\n",
      "Ohio       Cleveland            510               67                 50\n",
      "state       city                            \n",
      "California  Los Angeles    Area code            408\n",
      "                           total_day_calls      116\n",
      "                           total_day_minutes    204\n",
      "            San Francisco  Area code            408\n",
      "                           total_day_calls      109\n",
      "                           total_day_minutes    287\n",
      "New York    New York       Area code            415\n",
      "                           total_day_calls       84\n",
      "                           total_day_minutes     84\n",
      "Ohio        Cleveland      Area code            510\n",
      "                           total_day_calls       67\n",
      "                           total_day_minutes     50\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn.csv')\n",
    "\n",
    "# Predefined list to use as index\n",
    "new_index = [['California', 'California', 'New York', 'Ohio'], \n",
    "             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]\n",
    "\n",
    "# Create a multi-level index using predefined new_index\n",
    "churn_new = pd.MultiIndex.from_arrays(new_index, names=['state', 'city'])\n",
    "\n",
    "# Print churn_new\n",
    "print(churn_new)\n",
    "\n",
    "# Predefined list to use as index\n",
    "new_index = [['California', 'California', 'New York', 'Ohio'], \n",
    "             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]\n",
    "\n",
    "# Create a multi-level index using predefined new_index\n",
    "churn_new = pd.MultiIndex.from_arrays(new_index, names=['state', 'city'])\n",
    "\n",
    "# Assign the new index to the churn index\n",
    "churn.index = churn_new\n",
    "\n",
    "# Print churn\n",
    "print(churn)\n",
    "\n",
    "# Predefined list to use as index\n",
    "new_index = [['California', 'California', 'New York', 'Ohio'], \n",
    "             ['Los Angeles', 'San Francisco', 'New York', 'Cleveland']]\n",
    "\n",
    "# Create a multi-level index using predefined new_index\n",
    "churn_new = pd.MultiIndex.from_arrays(new_index, names=['state', 'city'])\n",
    "\n",
    "# Assign the new index to the churn index\n",
    "churn.index = churn_new\n",
    "\n",
    "# Reshape by stacking churn DataFrame\n",
    "churn_stack = churn.stack()\n",
    "\n",
    "# Print churn_stack\n",
    "print(churn_stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone directory index\n",
    "\n",
    "After reshaping the dataset, you sent it to your colleagues and asked\n",
    "them to fill in some data. Now, they sent the new `churn` dataset back\n",
    "and you realized that its shape has changed.\n",
    "\n",
    "Before you go on, you need to do some reshaping again. The dataset\n",
    "contains a multi-level index in the columns. You'd like to have some\n",
    "columns set as the row index. Also, this time you will only stack some\n",
    "levels. You believe it will help you discover some patterns in the data.\n",
    "\n",
    "The DataFrame `churn` is available for you. It contains data about\n",
    "`state`, `city`, `total_day_calls` and `total_day_minutes` during `day`\n",
    "and `night` time. *Make sure to examine it in the console*!\n",
    "\n",
    "-   Set the columns `state` and `city` as the index of `churn`,\n",
    "    modifying the DataFrame in-place.\n",
    "-   Create a new `churn_stack` DataFrame by stacking the second column\n",
    "    level of the `churn` DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 day                     night              \n",
      "                         total calls total minutes total calls total minutes\n",
      "state      city                                                             \n",
      "California Los Angeles            85           107         116           204\n",
      "           San Francisco          90           167         109           287\n",
      "New York   New York               75            90          84            84\n",
      "Ohio       Cleveland              67           110          67            50\n",
      "                                        day  night\n",
      "state      city                                   \n",
      "California Los Angeles   total calls     85    116\n",
      "                         total minutes  107    204\n",
      "           San Francisco total calls     90    109\n",
      "                         total minutes  167    287\n",
      "New York   New York      total calls     75     84\n",
      "                         total minutes   90     84\n",
      "Ohio       Cleveland     total calls     67     67\n",
      "                         total minutes  110     50\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn_long.csv')\n",
    "churn = pd.pivot_table(churn, index=['state', 'city'], columns=['period', 'metric'], values='value')\n",
    "churn = churn.reset_index()\n",
    "churn.columns.names = (None, None)\n",
    "\n",
    "# Set state and city as index modifying the DataFrame\n",
    "churn.set_index(['state', 'city'], inplace=True)\n",
    "\n",
    "# Print churn\n",
    "print(churn)\n",
    "\n",
    "# edited/added\n",
    "churn = pd.read_csv('churn_long.csv')\n",
    "churn = pd.pivot_table(churn, index=['state', 'city'], columns=['period', 'metric'], values='value')\n",
    "churn = churn.reset_index()\n",
    "churn.columns.names = (None, None)\n",
    "\n",
    "# Set state and city as index modifying the DataFrame\n",
    "churn.set_index(['state', 'city'], inplace=True)\n",
    "\n",
    "# Reshape by stacking the second level\n",
    "churn_stack = churn.stack(level=1)\n",
    "\n",
    "# Print churn_stack\n",
    "print(churn_stack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text me!\n",
    "\n",
    "You are making progress in your customer's project. Now, you need to\n",
    "analyze a new dataset to find differences in the messages and gigabytes\n",
    "(GB) of data the customers use during the daytime and nighttime.\n",
    "\n",
    "To that aim, you will reshape your dataset `churn` using different\n",
    "levels. The advantage of your new dataset is that the column indices\n",
    "have names.\n",
    "\n",
    "The DataFrame `churn` is available for you. It contains data about\n",
    "`state`, `city`, `text messages` and `total GB` during `day` and `night`\n",
    "time.\n",
    "\n",
    "Reshape the `churn` DataFrame by stacking the `time` column level.\n",
    "Assign the reshaped DataFrame to `churn_time`.\n",
    "\n",
    "Now, define a reshaped DataFrame called `churn_feature` by stacking the\n",
    "`feature` column level of the `churn` DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature                         text_messages  total_GB\n",
      "state      city          time                          \n",
      "California Los Angeles   day               20         5\n",
      "                         night             30        10\n",
      "           San Francisco day               40         5\n",
      "                         night            100         5\n",
      "New York   New York      day               50         2\n",
      "                         night             20         9\n",
      "Ohio       Cleveland     day              100         3\n",
      "                         night             40         6\n",
      "time                                    day  night\n",
      "state      city          feature                  \n",
      "California Los Angeles   text_messages   20     30\n",
      "                         total_GB         5     10\n",
      "           San Francisco text_messages   40    100\n",
      "                         total_GB         5      5\n",
      "New York   New York      text_messages   50     20\n",
      "                         total_GB         2      9\n",
      "Ohio       Cleveland     text_messages  100     40\n",
      "                         total_GB         3      6\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn_long_v1.csv')\n",
    "churn = pd.pivot_table(churn, index=['state', 'city'], columns=['time', 'feature'], values='value')\n",
    "\n",
    "# Stack churn by the time column level\n",
    "churn_time = churn.stack(level='time')\n",
    "\n",
    "# Print churn_time\n",
    "print(churn_time)\n",
    "\n",
    "# Stack churn by the feature column level\n",
    "churn_feature = churn.stack(level='feature')\n",
    "\n",
    "# Print churn_feature\n",
    "print(churn_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unstacking DataFrames\n",
    "\n",
    "### International caller\n",
    "\n",
    "You have a new task. You will analyze the pattern of customers on\n",
    "international and domestic calls.\n",
    "\n",
    "You explore the `churn` dataset, which contains a multi-level row index.\n",
    "Again, you will reshape the data, as you expect it will help you to do\n",
    "further analysis.\n",
    "\n",
    "The DataFrame `churn` is available for you. It contains data about\n",
    "`minutes`, `calls`, and `charge` for different times of the day, types\n",
    "of calls, and exited status. *Make sure to examine it in the console*!\n",
    "\n",
    "Reshape the `churn` DataFrame by unstacking the last row level. Assign\n",
    "it to `churn_unstack`.\n",
    "\n",
    "Create a reshaped DataFrame called `churn_first` by unstacking the first\n",
    "row level of `churn`.\n",
    "\n",
    "Define a new DataFrame called `churn_second` by unstacking the second\n",
    "row level of `churn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time                 day                                                \n",
      "feature    text_messages                                     total_GB   \n",
      "city           Cleveland Los Angeles New York San Francisco Cleveland   \n",
      "state                                                                   \n",
      "California           NaN        20.0      NaN          40.0       NaN  \\\n",
      "New York             NaN         NaN     50.0           NaN       NaN   \n",
      "Ohio               100.0         NaN      NaN           NaN       3.0   \n",
      "\n",
      "time                                                  night               \n",
      "feature                                       text_messages               \n",
      "city       Los Angeles New York San Francisco     Cleveland Los Angeles   \n",
      "state                                                                     \n",
      "California         5.0      NaN           5.0           NaN        30.0  \\\n",
      "New York           NaN      2.0           NaN           NaN         NaN   \n",
      "Ohio               NaN      NaN           NaN          40.0         NaN   \n",
      "\n",
      "time                                                                            \n",
      "feature                            total_GB                                     \n",
      "city       New York San Francisco Cleveland Los Angeles New York San Francisco  \n",
      "state                                                                           \n",
      "California      NaN         100.0       NaN        10.0      NaN           5.0  \n",
      "New York       20.0           NaN       NaN         NaN      9.0           NaN  \n",
      "Ohio            NaN           NaN       6.0         NaN      NaN           NaN  \n",
      "time                    day                                            \n",
      "feature       text_messages                   total_GB                 \n",
      "state            California New York   Ohio California New York Ohio   \n",
      "city                                                                   \n",
      "Cleveland               NaN      NaN  100.0        NaN      NaN  3.0  \\\n",
      "Los Angeles            20.0      NaN    NaN        5.0      NaN  NaN   \n",
      "New York                NaN     50.0    NaN        NaN      2.0  NaN   \n",
      "San Francisco          40.0      NaN    NaN        5.0      NaN  NaN   \n",
      "\n",
      "time                  night                                          \n",
      "feature       text_messages                  total_GB                \n",
      "state            California New York  Ohio California New York Ohio  \n",
      "city                                                                 \n",
      "Cleveland               NaN      NaN  40.0        NaN      NaN  6.0  \n",
      "Los Angeles            30.0      NaN   NaN       10.0      NaN  NaN  \n",
      "New York                NaN     20.0   NaN        NaN      9.0  NaN  \n",
      "San Francisco         100.0      NaN   NaN        5.0      NaN  NaN  \n",
      "time                 day                                                \n",
      "feature    text_messages                                     total_GB   \n",
      "city           Cleveland Los Angeles New York San Francisco Cleveland   \n",
      "state                                                                   \n",
      "California           NaN        20.0      NaN          40.0       NaN  \\\n",
      "New York             NaN         NaN     50.0           NaN       NaN   \n",
      "Ohio               100.0         NaN      NaN           NaN       3.0   \n",
      "\n",
      "time                                                  night               \n",
      "feature                                       text_messages               \n",
      "city       Los Angeles New York San Francisco     Cleveland Los Angeles   \n",
      "state                                                                     \n",
      "California         5.0      NaN           5.0           NaN        30.0  \\\n",
      "New York           NaN      2.0           NaN           NaN         NaN   \n",
      "Ohio               NaN      NaN           NaN          40.0         NaN   \n",
      "\n",
      "time                                                                            \n",
      "feature                            total_GB                                     \n",
      "city       New York San Francisco Cleveland Los Angeles New York San Francisco  \n",
      "state                                                                           \n",
      "California      NaN         100.0       NaN        10.0      NaN           5.0  \n",
      "New York       20.0           NaN       NaN         NaN      9.0           NaN  \n",
      "Ohio            NaN           NaN       6.0         NaN      NaN           NaN  \n"
     ]
    }
   ],
   "source": [
    "# Reshape the churn DataFrame by unstacking\n",
    "churn_unstack = churn.unstack()\n",
    "\n",
    "# Print churn_unstack\n",
    "print(churn_unstack)\n",
    "\n",
    "# Reshape churn by unstacking the first row level\n",
    "churn_first = churn.unstack(level=0)\n",
    "\n",
    "# Print churn_zero\n",
    "print(churn_first)\n",
    "\n",
    "# Reshape churn by unstacking the second row level\n",
    "churn_second = churn.unstack(level=1)\n",
    "\n",
    "# Print churn_second\n",
    "print(churn_second)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call another time\n",
    "\n",
    "You discover some patterns when you reshaped the DataFrame. Now, you\n",
    "want to unstack the DataFrame again. This time you will choose which\n",
    "level to unstack and reorganize your indices.\n",
    "\n",
    "The same `churn` DataFrame is available for you. It contains data about\n",
    "`minutes`, `calls`, and `charge` for different times of the day, types\n",
    "of calls, and exited status.\n",
    "\n",
    "-   Reshape the `churn` DataFrame by unstacking the `time` level. Assign\n",
    "    it to `churn_time`.\n",
    "-   Now, sort the index of the resulting reshaped `churn` in descending\n",
    "    order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        calls               charge               minutes   \n",
      "time                      day    eve  night    day    eve  night     day   \n",
      "type          exited                                                       \n",
      "International churn      97.0    NaN   67.0  31.37    NaN  56.59   184.5  \\\n",
      "              no churn    NaN  117.0    NaN    NaN  20.28    NaN     NaN   \n",
      "National      churn     137.0    NaN    NaN  21.95    NaN    NaN   129.1   \n",
      "              no churn    NaN   88.0  103.0    NaN  23.31  18.77     NaN   \n",
      "\n",
      "                                      \n",
      "time                      eve  night  \n",
      "type          exited                  \n",
      "International churn       NaN  332.9  \n",
      "              no churn  119.3    NaN  \n",
      "National      churn       NaN    NaN  \n",
      "              no churn  137.1  110.4  \n",
      "                        calls               charge               minutes   \n",
      "time                      day    eve  night    day    eve  night     day   \n",
      "type          exited                                                       \n",
      "National      no churn    NaN   88.0  103.0    NaN  23.31  18.77     NaN  \\\n",
      "              churn     137.0    NaN    NaN  21.95    NaN    NaN   129.1   \n",
      "International no churn    NaN  117.0    NaN    NaN  20.28    NaN     NaN   \n",
      "              churn      97.0    NaN   67.0  31.37    NaN  56.59   184.5   \n",
      "\n",
      "                                      \n",
      "time                      eve  night  \n",
      "type          exited                  \n",
      "National      no churn  137.1  110.4  \n",
      "              churn       NaN    NaN  \n",
      "International no churn  119.3    NaN  \n",
      "              churn       NaN  332.9  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn_long_v2.csv')\n",
    "churn = pd.pivot_table(churn, index=['time', 'type', 'exited'], columns=['metric'], values='value')\n",
    "churn.columns.names = [None]\n",
    "\n",
    "# Unstack the time level from churn\n",
    "churn_time = churn.unstack(level='time')\n",
    "\n",
    "# Print churn_time\n",
    "print(churn_time)\n",
    "\n",
    "# Sort the index in descending order\n",
    "churn_time = churn.unstack(level='time').sort_index(ascending=False)\n",
    "\n",
    "# Print churn_time\n",
    "print(churn_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing your voicemail\n",
    "\n",
    "You will perform one final task before moving to a new project. You will\n",
    "reshape the DataFrame `churn` again. This time, you'll reorganize a row\n",
    "index as a column index. After that, you will move a column index to a\n",
    "row index. To do this, you will first unstack the DataFrame, and then\n",
    "stack it.\n",
    "\n",
    "The same `churn` DataFrame is available for you. It contains data about\n",
    "`minutes`, `calls`, and `charge` for different times of the day, types\n",
    "of calls, and exited status. *Make sure to examine it in the console*!\n",
    "\n",
    "-   Reshape `churn` by unstacking the `type` level. Assign it to\n",
    "    `churn_type`.\n",
    "-   Stack the resulting DataFrame using the first column level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type                    International  National\n",
      "time  exited                                   \n",
      "day   churn    calls            97.00    137.00\n",
      "               charge           31.37     21.95\n",
      "               minutes         184.50    129.10\n",
      "eve   no churn calls           117.00     88.00\n",
      "               charge           20.28     23.31\n",
      "               minutes         119.30    137.10\n",
      "night churn    calls            67.00       NaN\n",
      "               charge           56.59       NaN\n",
      "               minutes         332.90       NaN\n",
      "      no churn calls              NaN    103.00\n",
      "               charge             NaN     18.77\n",
      "               minutes            NaN    110.40\n"
     ]
    }
   ],
   "source": [
    "# Unstack churn by type level\n",
    "churn_type = churn.unstack(level='type')\n",
    "\n",
    "# Stack churn_final using the first column level\n",
    "churn_final = churn_type.stack(level=0)\n",
    "\n",
    "# Print churn_final\n",
    "print(churn_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with multiple levels\n",
    "\n",
    "### Swap your SIM card\n",
    "\n",
    "Great job so far! You were able to reshape your dataset in several ways.\n",
    "Now it's time to go a step further and analyze the data to discover if a\n",
    "customer's cell phone plan is related to the customer leaving.\n",
    "\n",
    "You explore the `churn` dataset and notice that the row levels are not\n",
    "well organized. First, you want to rearrange your row indicesso it's\n",
    "easier to reshape your DataFrame.\n",
    "\n",
    "The `churn` DataFrame is available for you. It contains data about\n",
    "`minutes`, `voicemail`, and `data` plans for different years. The data\n",
    "is indexed by `state`, `city`, and `exited` status. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "-   Switch the first and the third row index levels of `churn`. Assign\n",
    "    it to `churn_swap`.\n",
    "-   Reshape the `churn` DataFrame by unstacking the last level. Assign\n",
    "    it to `churn_unstack`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              calls  charge  minutes\n",
      "exited   type          time                         \n",
      "churn    International day     97.0   31.37    184.5\n",
      "         National      day    137.0   21.95    129.1\n",
      "no churn International eve    117.0   20.28    119.3\n",
      "         National      eve     88.0   23.31    137.1\n",
      "churn    International night   67.0   56.59    332.9\n",
      "no churn National      night  103.0   18.77    110.4\n",
      "                        calls               charge               minutes   \n",
      "time                      day    eve  night    day    eve  night     day   \n",
      "exited   type                                                              \n",
      "churn    International   97.0    NaN   67.0  31.37    NaN  56.59   184.5  \\\n",
      "         National       137.0    NaN    NaN  21.95    NaN    NaN   129.1   \n",
      "no churn International    NaN  117.0    NaN    NaN  20.28    NaN     NaN   \n",
      "         National         NaN   88.0  103.0    NaN  23.31  18.77     NaN   \n",
      "\n",
      "                                      \n",
      "time                      eve  night  \n",
      "exited   type                         \n",
      "churn    International    NaN  332.9  \n",
      "         National         NaN    NaN  \n",
      "no churn International  119.3    NaN  \n",
      "         National       137.1  110.4  \n"
     ]
    }
   ],
   "source": [
    "# Switch the first and third row index levels in churn\n",
    "churn_swap = churn.swaplevel(0, 2)\n",
    "\n",
    "# Print churn_swap\n",
    "print(churn_swap)\n",
    "\n",
    "# Switch the first and third row index levels in churn\n",
    "churn_swap = churn.swaplevel(0, 2)\n",
    "\n",
    "# Reshape by unstacking the last row level \n",
    "churn_unstack = churn_swap.unstack()\n",
    "\n",
    "# Print churn_unstack\n",
    "print(churn_unstack)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two many calls\n",
    "\n",
    "Your last analysis was successful, but you still have some questions to\n",
    "answer. You are not satisfied with the organization of the data in your\n",
    "DataFrame.\n",
    "\n",
    "For that reason, you plan on switching and rearranging row and column\n",
    "indices by chaining the stacking and unstacking processes. Also, you\n",
    "would like to rearrange several levels at the same time.\n",
    "\n",
    "The same `churn` DataFrame is available for you. It contains data about\n",
    "`minutes`, `voicemail`, and `data` plans for different years. The data\n",
    "is indexed by `state`, `city`, and `exited` status.\n",
    "\n",
    "-   Create a new DataFrame called `churn_unstack` by unstacking the\n",
    "    first and second row levels of the DataFrame `churn`.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Stack the resulting DataFrame using the `plan` and `year` column\n",
    "    levels in that order. Assign it to `churn_py`.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Switch the first and second column levels in the resulting\n",
    "    DataFrame. Assign it to `churn_switch`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year              2019                                                    \n",
      "plan              data                                 minutes            \n",
      "exited           churn            no_churn               churn            \n",
      "state       California New York California New York California New York   \n",
      "city                                                                      \n",
      "Los Angeles        2.0      NaN        3.0      NaN        0.0      NaN  \\\n",
      "New York           NaN      5.0        NaN      4.0        NaN      1.0   \n",
      "\n",
      "year                                                 ...       2020            \n",
      "plan                             voicemail           ...       data            \n",
      "exited        no_churn               churn           ...   no_churn            \n",
      "state       California New York California New York  ... California New York   \n",
      "city                                                 ...                       \n",
      "Los Angeles        0.0      NaN        1.0      NaN  ...        2.0      NaN  \\\n",
      "New York           NaN      1.0        NaN      0.0  ...        NaN      6.0   \n",
      "\n",
      "year                                                                      \n",
      "plan           minutes                               voicemail            \n",
      "exited           churn            no_churn               churn            \n",
      "state       California New York California New York California New York   \n",
      "city                                                                      \n",
      "Los Angeles        1.0      NaN        1.0      NaN        1.0      NaN  \\\n",
      "New York           NaN      0.0        NaN      1.0        NaN      1.0   \n",
      "\n",
      "year                             \n",
      "plan                             \n",
      "exited        no_churn           \n",
      "state       California New York  \n",
      "city                             \n",
      "Los Angeles        0.0      NaN  \n",
      "New York           NaN      0.0  \n",
      "\n",
      "[2 rows x 24 columns]\n",
      "exited                          churn            no_churn         \n",
      "state                      California New York California New York\n",
      "city        plan      year                                        \n",
      "Los Angeles data      2019        2.0      NaN        3.0      NaN\n",
      "                      2020        5.0      NaN        2.0      NaN\n",
      "            minutes   2019        0.0      NaN        0.0      NaN\n",
      "                      2020        1.0      NaN        1.0      NaN\n",
      "            voicemail 2019        1.0      NaN        1.0      NaN\n",
      "                      2020        1.0      NaN        0.0      NaN\n",
      "New York    data      2019        NaN      5.0        NaN      4.0\n",
      "                      2020        NaN      2.0        NaN      6.0\n",
      "            minutes   2019        NaN      1.0        NaN      1.0\n",
      "                      2020        NaN      0.0        NaN      1.0\n",
      "            voicemail 2019        NaN      0.0        NaN      0.0\n",
      "                      2020        NaN      1.0        NaN      0.0\n",
      "state                      California New York California New York\n",
      "exited                          churn    churn   no_churn no_churn\n",
      "city        plan      year                                        \n",
      "Los Angeles data      2019        2.0      NaN        3.0      NaN\n",
      "                      2020        5.0      NaN        2.0      NaN\n",
      "            minutes   2019        0.0      NaN        0.0      NaN\n",
      "                      2020        1.0      NaN        1.0      NaN\n",
      "            voicemail 2019        1.0      NaN        1.0      NaN\n",
      "                      2020        1.0      NaN        0.0      NaN\n",
      "New York    data      2019        NaN      5.0        NaN      4.0\n",
      "                      2020        NaN      2.0        NaN      6.0\n",
      "            minutes   2019        NaN      1.0        NaN      1.0\n",
      "                      2020        NaN      0.0        NaN      1.0\n",
      "            voicemail 2019        NaN      0.0        NaN      0.0\n",
      "                      2020        NaN      1.0        NaN      0.0\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn_long_v3.csv')\n",
    "churn = pd.pivot_table(churn, index=['exited', 'state', 'city'], columns=['year', 'plan'], values='value')\n",
    "\n",
    "# Unstack the first and second row level of churn\n",
    "churn_unstack = churn.unstack(level=[0, 1])\n",
    "\n",
    "# Print churn_unstack\n",
    "print(churn_unstack)\n",
    "\n",
    "# Unstack the first and second row level of churn\n",
    "churn_unstack = churn.unstack(level=[0, 1])\n",
    "\n",
    "# Stack the resulting DataFrame using plan and year\n",
    "churn_py = churn_unstack.stack(['plan', 'year'])\n",
    "\n",
    "# Print churn_py\n",
    "print(churn_py)\n",
    "\n",
    "# Unstack the first and second row level of churn\n",
    "churn_unstack = churn.unstack(level=[0, 1])\n",
    "\n",
    "# Stack the resulting DataFrame using plan and year\n",
    "churn_py = churn_unstack.stack(['plan', 'year'])\n",
    "\n",
    "# Switch the first and second column levels\n",
    "churn_switch = churn_py.swaplevel(0, 1, axis=1)\n",
    "\n",
    "# Print churn_switch\n",
    "print(churn_switch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling missing data\n",
    "\n",
    "### A missed phone call\n",
    "\n",
    "You finished reshaping your `churn` dataset in the previous exercises.\n",
    "Now, it is ready to be used. You remember that something caught your\n",
    "attention. You are sure you saw a clear pattern in the data.\n",
    "\n",
    "Before you fit a classification model, you decide to do something\n",
    "simpler. You want to see what else you can learn from the data. You will\n",
    "reshape your data by unstacking levels, but you know this process will\n",
    "generate missing data that you need to handle.\n",
    "\n",
    "The `churn` DataFrame contains different features of customers located\n",
    "in Los Angeles and New York, and is available for you. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "-   Reshape the `churn` DataFrame by unstacking the level named `churn`,\n",
    "    filling the missing values with zero.\n",
    "-   Sort the `churn` DataFrame by the `voice_mail_plan` column in\n",
    "    descending order, then by `international_plan` column in ascending\n",
    "    order.\n",
    "-   Print the final `churn_sorted` DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         total_day_calls          \n",
      "churn                                              False  True    \n",
      "state international_plan voice_mail_plan                          \n",
      "LA    No                 Yes                     100.000    0.0  \\\n",
      "NY    No                 Yes                     115.000    0.0   \n",
      "LA    Yes                Yes                      71.000    0.0   \n",
      "NY    Yes                Yes                     120.000    0.0   \n",
      "LA    Yes                TRUE                     69.000    0.0   \n",
      "      No                 No                      106.818  100.0   \n",
      "NY    No                 No                       90.900   95.0   \n",
      "LA    Yes                No                       78.000    0.0   \n",
      "NY    Yes                No                      109.000   87.0   \n",
      "\n",
      "                                         total_night_calls         \n",
      "churn                                                False  True   \n",
      "state international_plan voice_mail_plan                           \n",
      "LA    No                 Yes                        84.250    0.0  \n",
      "NY    No                 Yes                       121.000    0.0  \n",
      "LA    Yes                Yes                       101.000    0.0  \n",
      "NY    Yes                Yes                        78.000    0.0  \n",
      "LA    Yes                TRUE                      104.000    0.0  \n",
      "      No                 No                         96.909  119.0  \n",
      "NY    No                 No                        100.800  101.5  \n",
      "LA    Yes                No                         90.000    0.0  \n",
      "NY    Yes                No                         99.000  113.0  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn_long_v4.csv')\n",
    "churn = pd.pivot_table(churn, index=['state', 'international_plan', 'voice_mail_plan', 'churn'], columns=['variable'], values='value')\n",
    "churn.columns.names = [None]\n",
    "\n",
    "# Unstack churn level and fill missing values with zero\n",
    "churn = churn.unstack(level='churn', fill_value=0)\n",
    "\n",
    "# Sort by descending voice mail plan and ascending international plan\n",
    "churn_sorted = churn.sort_index(level=[\"voice_mail_plan\", \"international_plan\"], \n",
    "                                ascending=[False, True])\n",
    "\n",
    "# Print final DataFrame and observe pattern\n",
    "print(churn_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't drop the stack\n",
    "\n",
    "It's almost time to go home, but first, you need to finish your last\n",
    "task. You have a small dataset containing the total number of calls made\n",
    "by customers.\n",
    "\n",
    "To perform your analysis, you need to reshape your `churn` data by\n",
    "stacking different levels. You know this process will generate missing\n",
    "data. You want to check if it is worth keeping the rows that contain all\n",
    "missing values, or if it's better to drop that information.\n",
    "\n",
    "The `churn` DataFrame is available for you.\n",
    "\n",
    "Reshape the `churn` DataFrame by stacking the `type` level. Then, fill\n",
    "the missing values generated with the value zero.\n",
    "\n",
    "Stack the `scope` level of `churn` without dropping the rows with\n",
    "missing values. Then, fill the missing values with zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scope                 International  National\n",
      "   type                                      \n",
      "CA Total Day Calls              8.0       0.0\n",
      "   Total Night Calls           34.0      24.0\n",
      "LA Total Day Calls             23.0       0.0\n",
      "   Total Night Calls           30.0       0.0\n",
      "NY Total Day Calls              8.0       0.0\n",
      "   Total Night Calls           34.0      24.0\n",
      "type              Total Day Calls  Total Night Calls\n",
      "   scope                                            \n",
      "CA International              8.0               34.0\n",
      "   National                   0.0               24.0\n",
      "LA International             23.0               30.0\n",
      "   National                   0.0                0.0\n",
      "NY International              8.0               34.0\n",
      "   National                   0.0               24.0\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "churn = pd.read_csv('churn_long_v5.csv')\n",
    "churn = pd.pivot_table(churn, index=['location'], columns=['type', 'scope'], values='value')\n",
    "churn.index.names = [None]\n",
    "\n",
    "# Stack the level type from churn\n",
    "churn_stack = churn.stack(level='type')\n",
    "\n",
    "# Fill the resulting missing values with zero\n",
    "churn_fill = churn_stack.fillna(0)\n",
    "\n",
    "# Print churn_fill \n",
    "print(churn_fill)\n",
    "\n",
    "# Stack the level scope without dropping rows with missing values\n",
    "churn_stack = churn.stack(level='scope', dropna=False)\n",
    "\n",
    "# Fill the resulting missing values with zero \n",
    "churn_fill = churn_stack.fillna(0)\n",
    "\n",
    "# Print churn_fill\n",
    "print(churn_fill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Reshaping\n",
    "\n",
    "You'll finish by learning how to combine the reshaping process with grouping to produce quick data manipulations. Lastly, you'll discover how to transform list-like columns and handle complex nested data, such as nested JSON files.\n",
    "\n",
    "## Reshaping and combining data\n",
    "\n",
    "### Less fast food, please!\n",
    "\n",
    "Monday again! You will start working on a new project - analyzing the\n",
    "evolution of obesity through the years. You have a dataset called\n",
    "`obesity` with the percentage of obesity in different countries and\n",
    "years. The data is also disaggregated by biological sex.\n",
    "\n",
    "Your main goals are to get the mean percentage of obesity by year and\n",
    "sex, and by country and sex. Also, you want to get the difference\n",
    "between years.\n",
    "\n",
    "You notice that the dataset has multiple indices, so you know you will\n",
    "have to unstack levels to accomplish your goal.\n",
    "\n",
    "The `obesity` DataFrame is available in your session.\n",
    "\n",
    "Reshape the `obesity` DataFrame by unstacking the first level, then get\n",
    "the mean value of the columns.\n",
    "\n",
    "Define an `obesity_mean` DataFrame by unstacking the second level of\n",
    "`obesity` and getting the mean value for the columns.\n",
    "\n",
    "Lastly, unstack the third level of the `obesity` DataFrame, then get the\n",
    "difference between the columns using `.diff()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biological_sex  year\n",
      "Female          2005    15.133333\n",
      "                2015    18.100000\n",
      "Male            2005    13.866667\n",
      "                2015    18.133333\n",
      "dtype: float64\n",
      "country    year\n",
      "Argentina  2005    22.85\n",
      "           2015    27.65\n",
      "Japan      2005     2.55\n",
      "           2015     4.10\n",
      "Norway     2005    18.10\n",
      "           2015    22.60\n",
      "dtype: float64\n",
      "                         perc_obesity     \n",
      "year                             2005 2015\n",
      "country   biological_sex                  \n",
      "Argentina Female                  NaN  4.3\n",
      "          Male                    NaN  5.3\n",
      "Japan     Female                  NaN  1.0\n",
      "          Male                    NaN  2.1\n",
      "Norway    Female                  NaN  3.6\n",
      "          Male                    NaN  5.4\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "obesity = pd.read_csv('obesity.csv')\n",
    "obesity.set_index(['country', 'biological_sex', 'year'], inplace=True)\n",
    "\n",
    "# Unstack the first level and calculate the mean of the columns\n",
    "obesity_general = obesity.unstack(level=0).mean(axis=1)\n",
    "\n",
    "# Print obesity_general\n",
    "print(obesity_general)\n",
    "\n",
    "# Unstack the second level and calculate the mean of the columns\n",
    "obesity_mean = obesity.unstack(level=1).mean(axis=1)\n",
    "\n",
    "# Print obesity_mean\n",
    "print(obesity_mean)\n",
    "\n",
    "# Unstack the third level and calculate the difference between columns\n",
    "obesity_variation = obesity.unstack(level=2).diff(axis=1)\n",
    "\n",
    "# Print obesity_variation\n",
    "print(obesity_variation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only going up\n",
    "\n",
    "After your last analysis, you are excited to keep working with the\n",
    "`obesity` dataset. You have added an extra column, the `variation`\n",
    "column, which indicates the range in which the percentage varies through\n",
    "regions in the same country. You are not sure if the mean is the best\n",
    "metric to summarize obesity levels.\n",
    "\n",
    "So you decide to explore the median percentage and variation of obesity\n",
    "by year and biological sex. Also, you want to get the maximum percentage\n",
    "observed by country, year, and biological sex.\n",
    "\n",
    "The DataFrame `obesity` is available in your session. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "Stack the `obesity` DataFrame, get the median value of the columns, and\n",
    "finally, unstack it again.\n",
    "\n",
    "Stack `obesity` by the first level, get the sum of the columns, and\n",
    "finally, unstack the DataFrame by the second level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        perc_obesity  variation\n",
      "country biological_sex                         \n",
      "France  Female                  18.1        8.2\n",
      "        Male                    16.9        8.4\n",
      "Germany Female                  17.2        5.2\n",
      "        Male                    18.7        5.9\n",
      "biological_sex  Female  Male\n",
      "country year                \n",
      "France  1995      23.0  20.4\n",
      "        2005      26.3  25.3\n",
      "        2015      32.1  33.3\n",
      "Germany 1995      19.0  19.5\n",
      "        2005      22.4  24.6\n",
      "        2015      28.5  33.4\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "obesity = pd.read_csv('obesity_v1.csv')\n",
    "obesity = pd.pivot_table(obesity, index=['country', 'biological_sex'], columns=['year', 'metrics'], values='value')\n",
    "obesity.columns.names = ['year', None]\n",
    "\n",
    "# Stack obesity, get median of columns and unstack again\n",
    "median_obesity = obesity.stack().median(axis=1).unstack()\n",
    "\n",
    "# Print median_obesity\n",
    "print(median_obesity)\n",
    "\n",
    "# Stack the first level, get sum, and unstack the second level\n",
    "obesity_sum = obesity.stack(level=0).sum(axis=1).unstack(level=1)\n",
    "\n",
    "# Print obesity_max\n",
    "print(obesity_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A group analysis\n",
    "\n",
    "You are almost done working for the day, but there is an extra analysis\n",
    "you want to do. You want to know if the mean and median percentage of\n",
    "obesity by country are different.\n",
    "\n",
    "You analyze the DataFrame `obesity`. You realize that `country` is part\n",
    "of the column labels, so you need to reshape the DataFrame so `country`\n",
    "is part of the index.\n",
    "\n",
    "You want to take a different approach. You will perform the desired\n",
    "calculations, combining the stacking process and `.groupby()` function.\n",
    "\n",
    "The `obesity` DataFrame is available in your session. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "Stack the `country` level of `obesity`, group it by `country`, and take\n",
    "the mean of all the columns.\n",
    "\n",
    "Stack the `country` level of `obesity`, group by `country`, and take the\n",
    "median of all the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           perc_obesity\n",
      "country                \n",
      "Argentina     23.000000\n",
      "Brazil        16.733333\n",
      "France        17.566667\n",
      "           perc_obesity\n",
      "country                \n",
      "Argentina         22.85\n",
      "Brazil            16.65\n",
      "France            17.50\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "obesity = pd.read_csv('obesity_v2.csv')\n",
    "obesity = pd.pivot_table(obesity, index=['year', 'biological_sex'], columns=['metric', 'country'], values='value')\n",
    "obesity.columns.names = [None, 'country']\n",
    "\n",
    "# Stack country level, group by country and get the mean \n",
    "obesity_mean = obesity.stack(level='country').groupby('country').mean()\n",
    "\n",
    "# Print obesity_mean\n",
    "print(obesity_mean)\n",
    "\n",
    "# Stack country level, group by country and get the median \n",
    "obesity_median = obesity.stack(level='country').groupby('country').median()\n",
    "\n",
    "# Print obesity_mean\n",
    "print(obesity_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming a list-like column\n",
    "\n",
    "### Merge it all\n",
    "\n",
    "Time to keep working with the obesity project! You will analyze the mean\n",
    "obesity percentage in different countries, but this time, the `obesity`\n",
    "DataFrame has a new column named `bounds`. It contains the minimum and\n",
    "maximum values you can find in different parts of the same country.\n",
    "\n",
    "You notice that these values are given in a list, so you decide that you\n",
    "need to transform that column. You would like to have each element in a\n",
    "new row.\n",
    "\n",
    "The DataFrame `obesity` is available in your session. *Make sure to\n",
    "examine it in the console*!\n",
    "\n",
    "-   Explode the values of the list-like column `bounds` of the `obesity`\n",
    "    DataFrame to a separate row.\n",
    "-   Merge the resulting Series with the `country` and `perc_obesity`\n",
    "    columns from the original `obesity` DataFrame. Use the indexes to\n",
    "    perform the operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [15.4, 31.5]\n",
      "1    [16.2, 32.4]\n",
      "2      [1.1, 3.5]\n",
      "3    [13.1, 33.0]\n",
      "Name: bounds, dtype: object\n",
      "     country  perc_obesity        bounds\n",
      "0  Argentina          21.5  [15.4, 31.5]\n",
      "1    Germany          22.3  [16.2, 32.4]\n",
      "2      Japan           2.5    [1.1, 3.5]\n",
      "3     Norway          23.0  [13.1, 33.0]\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "obesity = pd.read_csv('obesity_list.csv')\n",
    "\n",
    "# Explode the values of bounds to a separate row\n",
    "obesity_bounds = obesity['bounds'].explode()\n",
    "\n",
    "# Print obesity_bounds\n",
    "print(obesity_bounds)\n",
    "\n",
    "# Explode the values of bounds to a separate row\n",
    "obesity_bounds = obesity['bounds'].explode()\n",
    "\n",
    "# Merge obesity_bounds with country and perc_obesity columns of obesity using the indexes\n",
    "obesity_final = obesity[['country', 'perc_obesity']].merge(obesity_bounds, \n",
    "                                                           right_index=True, \n",
    "                                                           left_index=True)\n",
    "\n",
    "# Print obesity_final\n",
    "print(obesity_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode the bounds\n",
    "\n",
    "You were able to transform the list-like column successfully, but you\n",
    "are not satisfied with the steps you had to take. You want to find an\n",
    "easier way to get the same reshaped DataFrame.\n",
    "\n",
    "You remembered what you learned about exploding list-like columns, and\n",
    "you will apply a new strategy.\n",
    "\n",
    "The same DataFrame `obesity` is available in your session. It contains\n",
    "the `country`, `perc_obesity`, and the column `bounds` with the minimum\n",
    "and maximum values you can find in different parts of the same country.\n",
    "\n",
    "-   Transform the list-like column `bounds` in the DataFrame `obesity`\n",
    "    to get its elements in different rows.\n",
    "-   Modify the resulting DataFrame by resetting the index, dropping the\n",
    "    old one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     country  perc_obesity        bounds\n",
      "0  Argentina          21.5  [15.4, 31.5]\n",
      "1    Germany          22.3  [16.2, 32.4]\n",
      "2      Japan           2.5    [1.1, 3.5]\n",
      "3     Norway          23.0  [13.1, 33.0]\n"
     ]
    }
   ],
   "source": [
    "# Transform the list-like column named bounds \n",
    "obesity_explode = obesity.explode('bounds')\n",
    "\n",
    "# Modify obesity_explode by resetting the index\n",
    "obesity_explode.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print obesity_explode\n",
    "print(obesity_explode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The good old split\n",
    "\n",
    "You have to do one last task for the obesity project. Your colleague\n",
    "gave you a new dataset to analyze with which you will perform the same\n",
    "analysis as before.\n",
    "\n",
    "After inspecting the dataset `obesity`, you realize that you have the\n",
    "same columns as before, but the `bounds` column is not a list. This\n",
    "time, the column contains two values separated with a hyphen in the form\n",
    "of string.\n",
    "\n",
    "You will process the string and then transform the column.\n",
    "\n",
    "The DataFrame `obesity` is available in your session.\n",
    "\n",
    "-   Split the strings contained in the column `bounds`, using a hyphen\n",
    "    as the delimiter.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Now, assign the result of splitting the `bounds` column to the\n",
    "    `bounds` column of `obesity` .\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Transform the list-like column `bounds` in the resulting DataFrame\n",
    "    to get its elements in different rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [[15.4, 31.5]]\n",
      "1    [[16.2, 32.4]]\n",
      "2      [[1.1, 3.5]]\n",
      "3    [[13.1, 33.0]]\n",
      "Name: bounds, dtype: object\n",
      "     country  perc_obesity          bounds\n",
      "0  Argentina          21.5  [[15.4, 31.5]]\n",
      "1    Germany          22.3  [[16.2, 32.4]]\n",
      "2      Japan           2.5    [[1.1, 3.5]]\n",
      "3     Norway          23.0  [[13.1, 33.0]]\n",
      "     country  perc_obesity        bounds\n",
      "0  Argentina          21.5  [15.4, 31.5]\n",
      "1    Germany          22.3  [16.2, 32.4]\n",
      "2      Japan           2.5    [1.1, 3.5]\n",
      "3     Norway          23.0  [13.1, 33.0]\n"
     ]
    }
   ],
   "source": [
    "# Split the columns bounds using a hyphen as delimiter\n",
    "obesity_split = obesity['bounds'].str.split('-')\n",
    "\n",
    "# Print obesity_split\n",
    "print(obesity_split)\n",
    "\n",
    "# Assign the result of the split to the bounds column\n",
    "obesity_split = obesity.assign(bounds=obesity['bounds'].str.split('-'))\n",
    "\n",
    "# Print obesity_split\n",
    "print(obesity_split)\n",
    "\n",
    "# Transform the column bounds in the obesity DataFrame\n",
    "obesity_split = obesity.assign(bounds=obesity['bounds'].str.split('-')).explode('bounds')\n",
    "\n",
    "# Print obesity_split\n",
    "print(obesity_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading nested data into a DataFrame\n",
    "\n",
    "### Nested movies\n",
    "\n",
    "You are curious about a `movies` dataset you've had on your computer for\n",
    "some time now that contains data about different movies. You would like\n",
    "to analyze that data, but you realize it's in a nested JSON format.\n",
    "\n",
    "To read it into a DataFrame, you will need to use the function you have\n",
    "just learned. After that, you will reshape the resulting DataFrame to\n",
    "make it easier to work with.\n",
    "\n",
    "The semi-structured JSON named `movies` is available for you. *Make sure\n",
    "to examine it in the console*!\n",
    "\n",
    "-   Import the `json_normalize()` function from `pandas`.\n",
    "-   Normalize the JSON contained in `movies`. Separate the names\n",
    "    generated from nested records with an underscore.\n",
    "-   Reshape the resulting `movies_norm` DataFrame from wide to long\n",
    "    format, using the `director` and `producer` columns as unique\n",
    "    indexes. Name the new variable created from the columns `movies`,\n",
    "    starting with `features`, separated by an underscore with a suffix\n",
    "    containing words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                features\n",
      "director    producer      movies                        \n",
      "Woody Allen Letty Aronson title   Magic in the Moonlight\n",
      "                          year                      2014\n",
      "Niki Caro   Jason Reed    title                    Mulan\n",
      "                          year                      2020\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "movies = [\n",
    "  {'director': 'Woody Allen',\n",
    "  'producer': 'Letty Aronson',\n",
    "  'features': {'title': 'Magic in the Moonlight', 'year': 2014}},\n",
    "  {'director': 'Niki Caro',\n",
    "  'producer': 'Jason Reed',\n",
    "  'features': {'title': 'Mulan', 'year': 2020}}\n",
    "]\n",
    "  \n",
    "# Import the json_normalize function\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Normalize movies and separate the new columns with an underscore\n",
    "movies_norm = json_normalize(movies, sep='_')\n",
    "\n",
    "# Reshape using director and producer as index, create movies from column starting from features\n",
    "movies_long = pd.wide_to_long(movies_norm, stubnames='features', \n",
    "                              i=['director', 'producer'], j='movies', \n",
    "                              sep='_', suffix='\\w+')\n",
    "\n",
    "# Print movies_long\n",
    "print(movies_long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A complex film\n",
    "\n",
    "You are surprised by your findings from analyzing the `movies` data, so\n",
    "you find more data to continue the analysis. You explore the data and\n",
    "realize it's in a nested JSON format again.\n",
    "\n",
    "But this time, it's more complex. You would like to read it into a\n",
    "DataFrame. You will take several steps to achieve that.\n",
    "\n",
    "The semi-structured JSON named `movies` is available for you. *Make sure\n",
    "to examine it in the console*!\n",
    "\n",
    "The required function for this exercise has been pre-loaded.\n",
    "\n",
    "-   Normalize the semi-structured JSON contained in the variable\n",
    "    `movies`.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Specify that the column called `features` is the place where the\n",
    "    list of records is held.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Specify that `director` and `producer` columns should be used as\n",
    "    metadata for each record in the resulting DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      director       producer   \n",
      "0  Woody Allen  Letty Aronson  \\\n",
      "1    Niki Caro     Jason Reed   \n",
      "\n",
      "                                            features  \n",
      "0  [{'title': 'Magic in the Moonlight', 'year': 2...  \n",
      "1                 [{'title': 'Mulan', 'year': 2020}]  \n",
      "                      title  year\n",
      "0    Magic in the Moonlight  2014\n",
      "1  Vicky Cristina Barcelona  2008\n",
      "2         Midnight in Paris  2011\n",
      "3                     Mulan  2020\n",
      "                      title  year     director       producer\n",
      "0    Magic in the Moonlight  2014  Woody Allen  Letty Aronson\n",
      "1  Vicky Cristina Barcelona  2008  Woody Allen  Letty Aronson\n",
      "2         Midnight in Paris  2011  Woody Allen  Letty Aronson\n",
      "3                     Mulan  2020    Niki Caro     Jason Reed\n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "movies = [\n",
    "  {'director': 'Woody Allen',\n",
    "  'producer': 'Letty Aronson',\n",
    "  'features': [{'title': 'Magic in the Moonlight', 'year': 2014},\n",
    "  {'title': 'Vicky Cristina Barcelona', 'year': 2008},\n",
    "  {'title': 'Midnight in Paris', 'year': 2011}]},\n",
    "  {'director': 'Niki Caro',\n",
    "  'producer': 'Jason Reed',\n",
    "  'features': [{'title': 'Mulan', 'year': 2020}]}\n",
    "]\n",
    "  \n",
    "# Normalize the JSON contained in movies\n",
    "normalize_movies = json_normalize(movies)\n",
    "\n",
    "# Print normalize_movies\n",
    "print(normalize_movies)\n",
    "\n",
    "# Specify the features column as the list of records \n",
    "normalize_movies = json_normalize(movies, \n",
    "                                  record_path='features')\n",
    "\n",
    "# Print normalize_movies\n",
    "print(normalize_movies)\n",
    "\n",
    "# Specify director and producer to use as metadata for each record\n",
    "normalize_movies = json_normalize(movies, \n",
    "                                  record_path='features', \n",
    "                                  meta=['director', 'producer'])\n",
    "\n",
    "# Print normalize_movies\n",
    "print(normalize_movies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with nested data columns\n",
    "\n",
    "### Un-nesting birds\n",
    "\n",
    "Finally, your job for the day is done, but your colleague asked you a\n",
    "last minute favor. A client has provided data about birds he wants to\n",
    "classify.\n",
    "\n",
    "You examine the data and realize that it's in a bad format - the list of\n",
    "birds is in one file, and the characteristics of the birds are in\n",
    "another.\n",
    "\n",
    "You manage to read the bird names into a list called `names`. You read\n",
    "the bird facts into another list called `bird_facts`, but this list\n",
    "contains dictionaries in string format.\n",
    "\n",
    "To have a usable DataFrame, you will need to perform several operations.\n",
    "\n",
    "Both the `names` and `bird_facts` lists are available in your session.\n",
    "*Make sure to examine it in the console*! The `json` module is\n",
    "pre-loaded.\n",
    "\n",
    "-   Convert the `names` and `bird_facts` lists into columns of a new\n",
    "    DataFrame called `birds`, using the same names.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Flatten out the dictionary contained in `bird_facts` column by\n",
    "    applying the `loads` method from `json` module to the column.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Remove the `bird_facts` column from the `birds` DataFrame.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Concatenate the columns of the `birds` and `data_split` DataFrames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              names                                         bird_facts\n",
      "0          Killdeer  {\"Size\":\"Large\", \"Color\": \"Golden brown\", \"Beh...\n",
      "1  Chipping Sparrow  {\"Size\":\"Small\", \"Color\": \"Gray-white\", \"Behav...\n",
      "2     Cedar Waxwing  {\"Size\":\"Small\", \"Color\": \"Gray-brown\", \"Behav...\n",
      "    Size         Color                       Behavior         Habitat\n",
      "0  Large  Golden brown      Runs swiftly along ground     Rocky areas\n",
      "1  Small    Gray-white                Often in flocks  Open woodlands\n",
      "2  Small    Gray-brown  Catch insects over open water           Parks\n",
      "              names\n",
      "0          Killdeer\n",
      "1  Chipping Sparrow\n",
      "2     Cedar Waxwing\n",
      "              names   Size         Color                       Behavior   \n",
      "0          Killdeer  Large  Golden brown      Runs swiftly along ground  \\\n",
      "1  Chipping Sparrow  Small    Gray-white                Often in flocks   \n",
      "2     Cedar Waxwing  Small    Gray-brown  Catch insects over open water   \n",
      "\n",
      "          Habitat  \n",
      "0     Rocky areas  \n",
      "1  Open woodlands  \n",
      "2           Parks  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "import json\n",
    "names = ['Killdeer', 'Chipping Sparrow', 'Cedar Waxwing']\n",
    "bird_facts = [\n",
    "  '{\"Size\":\"Large\", \"Color\": \"Golden brown\", \"Behavior\": \"Runs swiftly along ground\", \"Habitat\": \"Rocky areas\"}',\n",
    "  '{\"Size\":\"Small\", \"Color\": \"Gray-white\", \"Behavior\": \"Often in flocks\", \"Habitat\": \"Open woodlands\"}',\n",
    "  '{\"Size\":\"Small\", \"Color\": \"Gray-brown\", \"Behavior\": \"Catch insects over open water\", \"Habitat\": \"Parks\"}'\n",
    "]\n",
    " \n",
    "# Define birds reading names and bird_facts lists into names and bird_facts columns \n",
    "birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n",
    "\n",
    "# Print birds\n",
    "print(birds)\n",
    "\n",
    "# Define birds reading names and bird_facts lists into names and bird_facts columns \n",
    "birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n",
    "\n",
    "# Apply the function json.loads function to the bird_facts column\n",
    "data_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n",
    "\n",
    "# Print birds\n",
    "print(data_split)\n",
    "\n",
    "# Define birds reading names and bird_facts lists into names and bird_facts columns\n",
    "birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n",
    "\n",
    "# Apply to bird_facts column the function loads from json module\n",
    "data_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n",
    "\n",
    "# Remove the bird_facts column from birds\n",
    "birds = birds.drop(columns='bird_facts')\n",
    "\n",
    "# Print birds\n",
    "print(birds)\n",
    "\n",
    "# Define birds reading names and bird_facts lists into names and bird_facts columns\n",
    "birds = pd.DataFrame(dict(names=names, bird_facts=bird_facts))\n",
    "\n",
    "# Apply to bird_facts column the function loads from json module\n",
    "data_split = birds['bird_facts'].apply(json.loads).apply(pd.Series)\n",
    "\n",
    "# Remove the bird_facts column from birds\n",
    "birds = birds.drop(columns='bird_facts')\n",
    "\n",
    "# Concatenate the columns of birds and data_split\n",
    "birds = pd.concat([birds,  data_split], axis=1)\n",
    "\n",
    "# Print birds\n",
    "print(birds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't dump the bird\n",
    "\n",
    "You want to read the `birds` data into a DataFrame like you did in the\n",
    "previous exercise, but this time, you would like to try a different\n",
    "approach.\n",
    "\n",
    "You would like to have a code that you can reuse in this situations, so\n",
    "you want to establish the fastest strategy to convert it into a usable\n",
    "DataFrame. You think that working with the `json` format could speed up\n",
    "the process.\n",
    "\n",
    "The `birds` DataFrame is available for you.\n",
    "\n",
    "-   Flatten out the content of the `bird_facts` column by applying the\n",
    "    `loads` method from the `json` module to the column. Transform the\n",
    "    result into a list.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Convert the list contained in `birds_facts` into a JSON format.\n",
    "    Assign it to the `birds_dump` variable.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Now, read the JSON contained in `birds_dump` into a DataFrame.\n",
    "\n",
    "<!-- -->\n",
    "\n",
    "-   Concatenate the `names` column of the `birds` DataFrame with all\n",
    "    columns in `birds_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Size': 'Large', 'Color': 'Golden brown', 'Behavior': 'Runs swiftly along ground', 'Habitat': 'Rocky areas'}, {'Size': 'Small', 'Color': 'Gray-white', 'Behavior': 'Often in flocks', 'Habitat': 'Open woodlands'}, {'Size': 'Small', 'Color': 'Gray-brown', 'Behavior': 'Catch insects over open water', 'Habitat': 'Parks'}]\n",
      "[{\"Size\": \"Large\", \"Color\": \"Golden brown\", \"Behavior\": \"Runs swiftly along ground\", \"Habitat\": \"Rocky areas\"}, {\"Size\": \"Small\", \"Color\": \"Gray-white\", \"Behavior\": \"Often in flocks\", \"Habitat\": \"Open woodlands\"}, {\"Size\": \"Small\", \"Color\": \"Gray-brown\", \"Behavior\": \"Catch insects over open water\", \"Habitat\": \"Parks\"}]\n",
      "    Size         Color                       Behavior         Habitat\n",
      "0  Large  Golden brown      Runs swiftly along ground     Rocky areas\n",
      "1  Small    Gray-white                Often in flocks  Open woodlands\n",
      "2  Small    Gray-brown  Catch insects over open water           Parks\n",
      "              names   Size         Color                       Behavior   \n",
      "0          Killdeer  Large  Golden brown      Runs swiftly along ground  \\\n",
      "1  Chipping Sparrow  Small    Gray-white                Often in flocks   \n",
      "2     Cedar Waxwing  Small    Gray-brown  Catch insects over open water   \n",
      "\n",
      "          Habitat  \n",
      "0     Rocky areas  \n",
      "1  Open woodlands  \n",
      "2           Parks  \n"
     ]
    }
   ],
   "source": [
    "# edited/added\n",
    "birds = pd.read_csv('birds.csv')\n",
    "\n",
    "# Apply json.loads to the bird_facts column and transform it to a list\n",
    "birds_facts = birds['bird_facts'].apply(json.loads).to_list()\n",
    "\n",
    "# Print birds_facts\n",
    "print(birds_facts)\n",
    "\n",
    "# Apply json.loads to the bird_facts column and transform it to a list \n",
    "birds_facts = birds['bird_facts'].apply(json.loads).to_list()\n",
    "\n",
    "# Convert birds_facts into a JSON \n",
    "birds_dump = json.dumps(birds_facts)\n",
    "\n",
    "# Print birds_dump\n",
    "print(birds_dump)\n",
    "\n",
    "# Apply json.loads to the bird_facts column and transform it to a list \n",
    "birds_facts = birds['bird_facts'].apply(json.loads).to_list()\n",
    "\n",
    "# Convert birds_facts into a JSON \n",
    "birds_dump = json.dumps(birds_facts)\n",
    "\n",
    "# Read the JSON birds_dump into a DataFrame \n",
    "birds_df = pd.read_json(birds_dump)\n",
    "\n",
    "# Print birds_df\n",
    "print(birds_df)\n",
    "\n",
    "# Apply json.loads to the bird_facts column and transform it to a list\n",
    "birds_facts = birds['bird_facts'].apply(json.loads).to_list()\n",
    "\n",
    "# Convert birds_fact into a JSON \n",
    "birds_dump = json.dumps(birds_facts)\n",
    "\n",
    "# Read the JSON birds_dump into a DataFrame \n",
    "birds_df = pd.read_json(birds_dump)\n",
    "\n",
    "# Concatenate the 'names' column of birds with birds_df \n",
    "birds_final = pd.concat([birds['names'], birds_df], axis=1)\n",
    "\n",
    "# Print birds_final\n",
    "print(birds_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final reshape\n",
    "\n",
    "### The final reshape\n",
    "\n",
    "You have reached the end of this course on reshaping data with pandas.\n",
    "\n",
    "### Congrats!\n",
    "\n",
    "Congratulations! You have done a great job!\n",
    "\n",
    "### Chapter 1\n",
    "\n",
    "You started by understanding the concepts of long and wide formats and the advantages of using each. Then, you learned how to reshape data using the pivot method, using columns as unique variables and the index as identifiers for individual observations. You created pivot tables and studied the difference from using pivot(), as well as the advantages and best practices for using each.\n",
    "\n",
    "### Chapter 2\n",
    "\n",
    "You then advanced, learning to convert a DataFrame from wide to long format using the melt method and the wide_to_long function. Also, you saw how to handle string columns or modify the index by splitting a string column into two columns or concatenating multiple string columns into one.\n",
    "\n",
    "### Chapter 3\n",
    "\n",
    "In Chapter 3, you explored what a multi-level index is and the advantage of using them. You also saw how to stack and unstack multi-level index DataFrames. Because these processes generated new missing data, you learned to handle that missing data using several approaches.\n",
    "\n",
    "### Chapter 4\n",
    "\n",
    "Finally, you found out how to combine the reshaping process with the grouping process to produce fast data manipulations. You also transformed list-like values contained in a column of a DataFrame to separate rows. You saw the basic structure of JSON format and learned how to read it into a DataFrame. Moreover, you discover how to handle nested data in columns. You have mastered the concepts of reshaping data with pandas. You can start applying them in your own projects.\n",
    "\n",
    "### Thank you!\n",
    "\n",
    "Lastly, thank you for staying with me to the end of this journey! I wish you all the best in your path of learning.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
