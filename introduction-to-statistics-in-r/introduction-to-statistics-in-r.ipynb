{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Course Description**\n",
                "\n",
                "Statistics is the study of how to collect, analyze, and draw conclusions from data. It’s a hugely valuable tool that you can use to bring the future into focus and infer the answer to tons of questions. For example, what is the likelihood of someone purchasing your product, how many calls will your support team receive, and how many jeans sizes should you manufacture to fit 95% of the population? In this course, you'll use sales data to discover how to answer questions like these as you grow your statistical skills and learn how to calculate averages, use scatterplots to show the relationship between numeric values, and calculate correlation. You'll also tackle probability, the backbone of statistical reasoning, and learn how to conduct a well-designed study to draw your own conclusions from data.\n",
                "\n",
                "# Summary Statistics\n",
                "\n",
                "Summary statistics gives you the tools you need to boil down massive datasets to reveal the highlights. In this chapter, you'll explore summary statistics including mean, median, and standard deviation, and learn how to accurately interpret them. You'll also develop your critical thinking skills, allowing you to choose the best summary statistics for your data.\n",
                "\n",
                "## What is statistics?\n",
                "\n",
                "### Descriptive and inferential statistics\n",
                "\n",
                "Statistics can be used to answer lots of different types of questions, but being able to identify which type of statistics is needed is essential to drawing accurate conclusions. In this exercise, you'll sharpen your skills by identifying which type is needed to answer each question.\n",
                "\n",
                "-   Identify which questions can be answered with descriptive statistics and which questions can be answered with inferential statistics.\n",
                "\n",
                "##### Descriptive\n",
                "\n",
                "- Given data on all 100,000 people who viewed an ad, what percent of\n",
                "people clicked on it?\n",
                "- Given data on every customer service request made, what's the average\n",
                "time it took to respond?\n",
                "\n",
                "\n",
                "##### Inferential\n",
                "\n",
                "- After interviewing 100 customers, what percent of *all* your customers\n",
                "are satisfied with your product?\n",
                "- Given data on 20 fish caught in a lake, what's the average weight of all\n",
                "fish in the lake?\n",
                "\n",
                "### Data type classification\n",
                "\n",
                "In the video, you learned about two main types of data: numeric and categorical. Numeric variables can be classified as either discrete or continuous, and categorical variables can be classified as either nominal or ordinal. These characteristics of a variable determine which ways of summarizing your data will work best.\n",
                "\n",
                "-   Map each variable to its data type by dragging and dropping.\n",
                "\n",
                "##### Continuous numeric\n",
                "\n",
                "- Air temperature\n",
                "- Kilowatts of electricity used\n",
                "\n",
                "##### Discrete numeric\n",
                "\n",
                "- Number of items in stock\n",
                "Number of clicks on an ad\n",
                "- Number of DataCamp courses taken\n",
                "\n",
                "##### Categorical\n",
                "\n",
                "- Brand of a product\n",
                "- Postal code\n",
                "\n",
                "## Measures of center\n",
                "\n",
                "### Mean and median\n",
                "\n",
                "In this chapter, you'll be working with the [2018 Food Carbon Footprint\n",
                "Index](https://www.nu3.de/blogs/nutrition/food-carbon-footprint-index-2018)\n",
                "from nu3. The `food_consumption` dataset contains information about the\n",
                "kilograms of food consumed per person per year in each country in each\n",
                "food category (`consumption`) as well as information about the carbon\n",
                "footprint of that food category (`co2_emissions`) measured in kilograms\n",
                "of carbon dioxide, or CO\\\\\\_2\\\\, per person per year in each country.\n",
                "\n",
                "In this exercise, you'll compute measures of center to compare food\n",
                "consumption in the US and Belgium using your `dplyr` skills.\n",
                "\n",
                "`dplyr` is loaded for you and `food_consumption` is available.\n",
                "\n",
                "-   Create two data frames: one that holds the rows of\n",
                "    `food_consumption` for `\"Belgium\"` and the another that holds rows\n",
                "    for `\"USA\"`. Call these `belgium_consumption` and `usa_consumption`.\n",
                "-   Calculate the mean and median of kilograms of food consumed per\n",
                "    person per year for both countries.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Filter `food_consumption` for rows with data about Belgium and the\n",
                "    USA.\n",
                "-   Group the filtered data by `country`.\n",
                "-   Calculate the mean and median of the kilograms of food consumed per\n",
                "    person per year in each country. Call these columns\n",
                "    `mean_consumption` and `median_consumption`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "library(tidyverse)\n",
                "food_consumption <- readRDS('food_consumption.rds')\n",
                "\n",
                "# Filter for Belgium\n",
                "belgium_consumption <- food_consumption %>%\n",
                "  filter(country == \"Belgium\")\n",
                "\n",
                "# Filter for USA\n",
                "usa_consumption <- food_consumption %>%\n",
                "  filter(country == \"USA\")\n",
                "\n",
                "# Calculate mean and median consumption in Belgium\n",
                "mean(belgium_consumption$consumption)\n",
                "median(belgium_consumption$consumption)\n",
                "\n",
                "# Calculate mean and median consumption in USA\n",
                "mean(usa_consumption$consumption)\n",
                "median(usa_consumption$consumption)\n",
                "\n",
                "food_consumption %>%\n",
                "  # Filter for Belgium and USA\n",
                "  filter(country %in% c(\"Belgium\", \"USA\")) %>%\n",
                "  # Group by country\n",
                "  group_by(country) %>%\n",
                "  # Get mean_consumption and median_consumption\n",
                "  summarize(mean_consumption = mean(consumption),\n",
                "           median_consumption = median(consumption))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Mean vs. median\n",
                "\n",
                "In the video, you learned that the mean is the sum of all the data\n",
                "points divided by the total number of data points, and the median is the\n",
                "middle value of the dataset where 50% of the data is less than the\n",
                "median, and 50% of the data is greater than the median. In this\n",
                "exercise, you'll compare these two measures of center.\n",
                "\n",
                "`dplyr` and `ggplot2` are loaded and `food_consumption` is available.\n",
                "\n",
                "-   Filter `food_consumption` to get the rows where `food_category` is\n",
                "    `\"rice\"`.\n",
                "-   Create a histogram using `ggplot2` of `co2_emission` for rice.\n",
                "\n",
                "Take a look at the histogram of the CO<sub>2<\/sub> emissions for rice\n",
                "you just plotted. Which of the following terms best describes the shape\n",
                "of the data?\n",
                "\n",
                "- [ ] No skew\n",
                "- [ ] Left-skewed\n",
                "- [x] Right-skewed\n",
                "\n",
                "-   Filter `food_consumption` to get the rows where `food_category` is `\"rice\"`.\n",
                "-   Summarize the data to get the mean and median of `co2_emission`, calling them `mean_co2` and `median_co2.`\n",
                "\n",
                "Given the skew of this data, what measure of central tendency best\n",
                "summarizes the kilograms of CO<sub>2<\/sub> emissions per person per year\n",
                "for rice?\n",
                "\n",
                "- [ ] Mean\n",
                "- [x] Median\n",
                "- [ ] Both mean and median\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "food_consumption %>%\n",
                "  # Filter for rice food category\n",
                "  filter(food_category == \"rice\") %>%\n",
                "  # Create histogram of co2_emission\n",
                "  ggplot(aes(co2_emission)) +\n",
                "    geom_histogram()\n",
                "\n",
                "food_consumption %>%\n",
                "  # Filter for rice food category\n",
                "  filter(food_category == \"rice\") %>%\n",
                "  # Create histogram of co2_emission\n",
                "  ggplot(aes(co2_emission)) +\n",
                "    geom_histogram()\n",
                "\n",
                "food_consumption %>%\n",
                "  # Filter for rice food category\n",
                "  filter(food_category == \"rice\") %>% \n",
                "  # Get mean_co2 and median_co2\n",
                "  summarize(mean_co2 = mean(co2_emission),\n",
                "            median_co2 = median(co2_emission))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Measures of spread\n",
                "\n",
                "### Quartiles, quantiles, and quintiles\n",
                "\n",
                "Quantiles are a great way of summarizing numerical data since they can\n",
                "be used to measure center and spread, as well as to get a sense of where\n",
                "a data point stands in relation to the rest of the dataset. For example,\n",
                "you might want to give a discount to the 10% most active users on a\n",
                "website.\n",
                "\n",
                "In this exercise, you'll calculate quartiles, quintiles, and deciles,\n",
                "which split up a dataset into 4, 5, and 10 pieces, respectively.\n",
                "\n",
                "The `dplyr` package is loaded and `food_consumption` is available.\n",
                "\n",
                "-   Calculate the quartiles of the `co2_emission` column of\n",
                "    `food_consumption`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the six quantiles that split up the data into 5 pieces\n",
                "    (quintiles) of the `co2_emission` column of `food_consumption`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the eleven quantiles of `co2_emission` that split up the\n",
                "    data into ten pieces (deciles).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate the quartiles of co2_emission\n",
                "quantile(food_consumption$co2_emission)\n",
                "\n",
                "# Calculate the quintiles of co2_emission\n",
                "quantile(food_consumption$co2_emission, probs = c(0, 0.2, 0.4, 0.6, 0.8, 1))\n",
                "\n",
                "# Calculate the deciles of co2_emission\n",
                "quantile(food_consumption$co2_emission, probs = seq(0, 1, 0.1))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Variance and standard deviation\n",
                "\n",
                "Variance and standard deviation are two of the most common ways to\n",
                "measure the spread of a variable, and you'll practice calculating these\n",
                "in this exercise. Spread is important since it can help inform\n",
                "expectations. For example, if a salesperson sells a mean of 20 products\n",
                "a day, but has a standard deviation of 10 products, there will probably\n",
                "be days where they sell 40 products, but also days where they only sell\n",
                "one or two. Information like this is important, especially when making\n",
                "predictions.\n",
                "\n",
                "Both `dplyr` and `ggplot2` are loaded, and `food_consumption` is\n",
                "available.\n",
                "\n",
                "-   Calculate the variance and standard deviation of `co2_emission` for\n",
                "    each `food_category` by grouping by and summarizing variance as\n",
                "    `var_co2` and standard deviation as `sd_co2`.\n",
                "-   Create a histogram of `co2_emission` for each `food_category` using\n",
                "    `facet_wrap()`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate variance and sd of co2_emission for each food_category\n",
                "food_consumption %>% \n",
                "  group_by(food_category) %>% \n",
                "  summarize(var_co2 = var(co2_emission),\n",
                "           sd_co2 = sd(co2_emission))\n",
                "\n",
                "# Create subgraphs for each food_category: histogram of co2_emission\n",
                "ggplot(food_consumption, aes(co2_emission)) +\n",
                "  # Create a histogram\n",
                "  geom_histogram() +\n",
                "  # Create a separate sub-graph for each food_category\n",
                "  facet_wrap(~ food_category)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Finding outliers using IQR\n",
                "\n",
                "Outliers can have big effects on statistics like mean, as well as\n",
                "statistics that rely on the mean, such as variance and standard\n",
                "deviation. Interquartile range, or IQR, is another way of measuring\n",
                "spread that's less influenced by outliers. IQR is also often used to\n",
                "find outliers. If a value is less than \\\\\\text{Q1} - 1.5 \\times\n",
                "\\text{IQR}\\\\ or greater than \\\\\\text{Q3} + 1.5 \\times \\text{IQR}\\\\, it's\n",
                "considered an outlier. In fact, this is how the lengths of the whiskers\n",
                "in a `ggplot2` box plot are calculated.\n",
                "\n",
                "![Diagram of a box plot showing median, quartiles, and\n",
                "outliers](https://assets.datacamp.com/production/repositories/5758/datasets/ca7e6e1832be7ec1842f62891815a9b0488efa83/Screen%20Shot%202020-04-28%20at%2010.04.54%20AM.png)\n",
                "\n",
                "In this exercise, you'll calculate IQR and use it to find some outliers.\n",
                "Both `dplyr` and `ggplot2` are loaded and `food_consumption` is\n",
                "available.\n",
                "\n",
                "-   Calculate the total `co2_emission` per country by grouping by\n",
                "    country and taking the sum of `co2_emission`. Call the sum\n",
                "    `total_emission` and store the resulting data frame as\n",
                "    `emissions_by_country`.\n",
                "-   Compute the first and third quartiles of `total_emission` and store these as `q1` and `q3`.\n",
                "-   Calculate the interquartile range of `total_emission` and store it as `iqr`.\n",
                "-   Calculate the lower and upper cutoffs for outliers of `total_emission`, and store these as `lower` and `upper`.\n",
                "-   Use `filter()` to get countries with a `total_emission` greater than the `upper` cutoff or a `total_emission` less than the `lower` cutoff.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate total co2_emission per country: emissions_by_country\n",
                "emissions_by_country <- food_consumption %>%\n",
                "  group_by(country) %>%\n",
                "  summarize(total_emission = sum(co2_emission))\n",
                "\n",
                "emissions_by_country\n",
                "\n",
                "# Calculate total co2_emission per country: emissions_by_country\n",
                "emissions_by_country <- food_consumption %>%\n",
                "  group_by(country) %>%\n",
                "  summarize(total_emission = sum(co2_emission))\n",
                "\n",
                "# Compute the first and third quartiles and IQR of total_emission\n",
                "q1 <- quantile(emissions_by_country$total_emission, 0.25)\n",
                "q3 <- quantile(emissions_by_country$total_emission, 0.75)\n",
                "iqr <- q3 - q1\n",
                "\n",
                "# Calculate total co2_emission per country: emissions_by_country\n",
                "emissions_by_country <- food_consumption %>%\n",
                "  group_by(country) %>%\n",
                "  summarize(total_emission = sum(co2_emission))\n",
                "\n",
                "# Compute the first and third quartiles and IQR of total_emission\n",
                "q1 <- quantile(emissions_by_country$total_emission, 0.25)\n",
                "q3 <- quantile(emissions_by_country$total_emission, 0.75)\n",
                "iqr <- q3 - q1\n",
                "\n",
                "# Calculate the lower and upper cutoffs for outliers\n",
                "lower <- q1 - 1.5 * iqr\n",
                "upper <- q3 + 1.5 * iqr\n",
                "\n",
                "# Calculate total co2_emission per country: emissions_by_country\n",
                "emissions_by_country <- food_consumption %>%\n",
                "  group_by(country) %>%\n",
                "  summarize(total_emission = sum(co2_emission))\n",
                "\n",
                "# Compute the first and third quartiles and IQR of total_emission\n",
                "q1 <- quantile(emissions_by_country$total_emission, 0.25)\n",
                "q3 <- quantile(emissions_by_country$total_emission, 0.75)\n",
                "iqr <- q3 - q1\n",
                "\n",
                "# Calculate the lower and upper cutoffs for outliers\n",
                "lower <- q1 - 1.5 * iqr\n",
                "upper <- q3 + 1.5 * iqr\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Random Numbers and Probability\n",
                "\n",
                "In this chapter, you'll learn how to generate random samples and measure chance using probability. You'll work with real-world sales data to calculate the probability of a salesperson being successful. Finally, you’ll use the binomial distribution to model events with binary outcomes.\n",
                "\n",
                "## What are the chances?\n",
                "\n",
                "### With or without replacement?\n",
                "\n",
                "In the video, you learned about two different ways of taking samples: with replacement and without replacement. Although it isn't always easy to tell which best fits various situations, it's important to correctly identify this so that any probabilities you report are accurate. In this exercise, you'll put your new knowledge to the test and practice figuring this out.\n",
                "\n",
                "-   For each scenario, decide whether it's sampling with replacement or sampling without replacement.\n",
                "\n",
                "##### With replacement\n",
                "\n",
                "- Flipping a coin 3 times\n",
                "- Rolling a die twice\n",
                "\n",
                "##### Without replacement\n",
                "\n",
                "- From a deck of cards, dealing 3 players 7 cards each\n",
                "- Randomly picking 3 people to work on the weekend from a group of 20\n",
                "people\n",
                "- Randomly selecting 5 products from the assembly line to test for quality\n",
                "assurance\n",
                "\n",
                "### Calculating probabilities\n",
                "\n",
                "You're in charge of the sales team, and it's time for performance\n",
                "reviews, starting with Amir. As part of the review, you want to randomly\n",
                "select a few of the deals that he's worked on over the past year so that\n",
                "you can look at them more deeply. Before you start selecting deals,\n",
                "you'll first figure out what the chances are of selecting certain deals.\n",
                "\n",
                "Recall that the probability of an event can be calculated by $$\n",
                "P(\\text{event}) = \\frac{\\text{# ways event can happen}}{\\text{total \\#\n",
                "of possible outcomes}} $$\n",
                "\n",
                "`dplyr` is loaded and `amir_deals` is available.\n",
                "\n",
                "-   Count the number of deals Amir worked on for each `product` type.\n",
                "-   Create a new column called `prob` by dividing `n` by the total number of deals Amir worked on.\n",
                "\n",
                "If you randomly select one of Amir's deals, what's the probability that the deal will involve `Product C`?\n",
                "\n",
                "\n",
                "- [ ] 15%\n",
                "- [ ] 80.43%\n",
                "- [x] 8.43%\n",
                "- [ ] 22.5%\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "amir_deals <- readRDS('seller_1.rds') %>% \n",
                "  select(product, client, status, amount, num_users)\n",
                "\n",
                "# Count the deals for each product\n",
                "amir_deals %>%\n",
                "  count(product)\n",
                "\n",
                "# Calculate probability of picking a deal with each product\n",
                "amir_deals %>%\n",
                "  count(product) %>%\n",
                "  mutate(prob = n/sum(n))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Sampling deals\n",
                "\n",
                "In the previous exercise, you counted the deals Amir worked on. Now it's\n",
                "time to randomly pick five deals so that you can reach out to each\n",
                "customer and ask if they were satisfied with the service they received.\n",
                "You'll try doing this both with and without replacement.\n",
                "\n",
                "Additionally, you want to make sure this is done randomly and that it\n",
                "can be reproduced in case you get asked how you chose the deals, so\n",
                "you'll need to set the random seed before sampling from the deals.\n",
                "\n",
                "`dplyr` is loaded and `amir_deals` is available.\n",
                "\n",
                "-   Set the random seed to `31`.\n",
                "-   Take a sample of 5 deals **without** replacement.\n",
                "-   Take a sample of 5 deals **with** replacement.\n",
                "\n",
                "What type of sampling is better to use for this situation?\n",
                "\n",
                "- [ ] With replacement\n",
                "- [ ] Without replacement\n",
                "- [ ] It doesn't matter\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seed to 31\n",
                "set.seed(31)\n",
                "\n",
                "# Sample 5 deals without replacement\n",
                "amir_deals %>%\n",
                "  sample_n(5)\n",
                "\n",
                "# Set random seed to 31\n",
                "set.seed(31)\n",
                "\n",
                "# Sample 5 deals with replacement\n",
                "amir_deals %>%\n",
                "  sample_n(5, replace = TRUE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Discrete distributions\n",
                "\n",
                "### Creating a probability distribution\n",
                "\n",
                "A new restaurant opened a few months ago, and the restaurant's\n",
                "management wants to optimize its seating space based on the size of the\n",
                "groups that come most often. On one night, there are 10 groups of people\n",
                "waiting to be seated at the restaurant, but instead of being called in\n",
                "the order they arrived, they will be called randomly. In this exercise,\n",
                "you'll investigate the probability of groups of different sizes getting\n",
                "picked first. Data on each of the ten groups is contained in the\n",
                "`restaurant_groups` data frame.\n",
                "\n",
                "Remember that expected value can be calculated by multiplying each\n",
                "possible outcome with its corresponding probability and taking the sum.\n",
                "The `restaurant_groups` data is available and `dplyr` and `ggplot2` are\n",
                "loaded.\n",
                "\n",
                "-   Create a histogram of the `group_size` column of\n",
                "    `restaurant_groups`, setting the number of bins to `5`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Count the number of each `group_size` in `restaurant_groups`, then\n",
                "    add a column called `probability` that contains the probability of\n",
                "    randomly selecting a group of each size. Store this in a new data\n",
                "    frame called `size_distribution`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the expected value of the `size_distribution`, which\n",
                "    represents the expected group size.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the probability of randomly picking a group of 4 or more\n",
                "    people by filtering and summarizing.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "restaurant_groups <- read.csv('restaurant_groups.csv')\n",
                "\n",
                "# Create a histogram of restaurant_groups\n",
                "ggplot(restaurant_groups, aes(group_size)) +\n",
                "  geom_histogram(bins = 5)\n",
                "\n",
                "# Create probability distribution\n",
                "size_distribution <- restaurant_groups %>%\n",
                "  # Count number of each group size\n",
                "  count(group_size) %>%\n",
                "  # Calculate probability\n",
                "  mutate(probability = n / sum(n))\n",
                "\n",
                "size_distribution\n",
                "\n",
                "# Create probability distribution\n",
                "size_distribution <- restaurant_groups %>%\n",
                "  count(group_size) %>%\n",
                "  mutate(probability = n / sum(n))\n",
                "\n",
                "# Calculate expected group size\n",
                "expected_val <- sum(size_distribution$group_size *\n",
                "                    size_distribution$probability)\n",
                "expected_val\n",
                "\n",
                "# Create probability distribution\n",
                "size_distribution <- restaurant_groups %>%\n",
                "  count(group_size) %>%\n",
                "  mutate(probability = n / sum(n))\n",
                "\n",
                "# Calculate probability of picking group of 4 or more\n",
                "size_distribution %>%\n",
                "  # Filter for groups of 4 or larger\n",
                "  filter(group_size >= 4) %>%\n",
                "  # Calculate prob_4_or_more by taking sum of probabilities\n",
                "  summarize(prob_4_or_more = sum(probability))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Identifying distributions\n",
                "\n",
                "# Identifying distributions\n",
                "\n",
                "Which sample is most likely to have been taken from a uniform\n",
                "distribution?\n",
                "\n",
                "![A: bell-shaped distribution, B: relatively flat distribution, C: lots\n",
                "of lower values, fewer high\n",
                "values](https://assets.datacamp.com/production/repositories/5758/datasets/bd64d4775ec28f36b081d92aa38a391033c03b8f/Screen%20Shot%202020-05-04%20at%204.35.58%20PM.png)\n",
                "\n",
                "- [ ] A\n",
                "- [x] B\n",
                "- [ ] C\n",
                "\n",
                "### Expected value vs. sample mean\n",
                "\n",
                "The app to the right will take a sample from a discrete uniform\n",
                "distribution, which includes the numbers 1 through 9, and calculate the\n",
                "sample's mean. You can adjust the size of the sample using the slider.\n",
                "Note that the expected value of this distribution is 5.\n",
                "\n",
                "A sample is taken, and you win twenty dollars if the sample's mean is\n",
                "less than 4. There's a catch: you get to pick the sample's size.\n",
                "\n",
                "Which sample size is ***most likely*** to win you the twenty dollars?\n",
                "\n",
                "- [x] 10\n",
                "- [ ] 100\n",
                "- [ ] 1000\n",
                "- [ ] 5000\n",
                "- [ ] 10000\n",
                "\n",
                "## Continuous distributions\n",
                "\n",
                "### Which distribution?\n",
                "\n",
                "At this point, you've learned about the two different variants of the\n",
                "uniform distribution: the discrete uniform distribution, and the\n",
                "continuous uniform distribution. In this exercise, you'll decide which\n",
                "situations follow which distribution.\n",
                "\n",
                "![Illustration of discrete and continuous uniform\n",
                "distributions](https://assets.datacamp.com/production/repositories/5758/datasets/fe928d4c840f66544c6228b8b755e9bf15361b9f/Screen%20Shot%202020-05-04%20at%205.18.16%20PM.png)\n",
                "\n",
                "-   Map each situation to the probability distribution it would best be modeled by.\n",
                "\n",
                "##### Discrete uniform\n",
                "\n",
                "- The outcome of rolling a 4-sided die.\n",
                "- The ticket number of a raffle winner, assuming there is one ticket for\n",
                "each number from 1 to 100.\n",
                "\n",
                "##### Continuous uniform\n",
                "\n",
                "- The time you'll have to wait for a geyser to erupt if you show up at a\n",
                "random time, knowing that the geyser erupts exactly every ten minutes.\n",
                "- The time of day a baby will be born.\n",
                "\n",
                "##### Other\n",
                "\n",
                "- The height of a random person.\n",
                "\n",
                "### Data back-ups\n",
                "\n",
                "The sales software used at your company is set to automatically back\n",
                "itself up, but no one knows exactly what time the back-ups happen. It is\n",
                "known, however, that back-ups happen exactly every 30 minutes. Amir\n",
                "comes back from sales meetings at random times to update the data on the\n",
                "client he just met with. He wants to know how long he'll have to wait\n",
                "for his newly-entered data to get backed up. Use your new knowledge of\n",
                "continuous uniform distributions to model this situation and answer\n",
                "Amir's questions.\n",
                "\n",
                "-   To model how long Amir will wait for a back-up using a continuous\n",
                "    uniform distribution, save his lowest possible wait time as `min`\n",
                "    and his longest possible wait time as `max`. Remember that back-ups\n",
                "    happen every 30 minutes.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the probability that Amir has to wait less than 5 minutes,\n",
                "    and store in a variable called `prob_less_than_5`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the probability that Amir has to wait more than 5 minutes,\n",
                "    and store in a variable called `prob_greater_than_5`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Calculate the probability that Amir has to wait between 10 and 20\n",
                "    minutes, and store in a variable called `prob_between_10_and_20`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Min and max wait times for back-up that happens every 30 min\n",
                "min <- 0\n",
                "max <- 30\n",
                "\n",
                "# Min and max wait times for back-up that happens every 30 min\n",
                "min <- 0\n",
                "max <- 30\n",
                "\n",
                "# Calculate probability of waiting less than 5 mins\n",
                "prob_less_than_5 <- punif(5, min, max)\n",
                "prob_less_than_5\n",
                "\n",
                "# Min and max wait times for back-up that happens every 30 min\n",
                "min <- 0\n",
                "max <- 30\n",
                "\n",
                "# Calculate probability of waiting more than 5 mins\n",
                "prob_greater_than_5 <- punif(5, min, max, lower.tail = FALSE)\n",
                "prob_greater_than_5\n",
                "\n",
                "# Min and max wait times for back-up that happens every 30 min\n",
                "min <- 0\n",
                "max <- 30\n",
                "\n",
                "# Calculate probability of waiting 10-20 mins\n",
                "prob_between_10_and_20 <- punif(20, min, max) - punif(10, min, max)\n",
                "prob_between_10_and_20\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Simulating wait times\n",
                "\n",
                "To give Amir a better idea of how long he'll have to wait, you'll\n",
                "simulate Amir waiting 1000 times and create a histogram to show him what\n",
                "he should expect. Recall from the last exercise that his minimum wait\n",
                "time is 0 minutes and his maximum wait time is 30 minutes.\n",
                "\n",
                "A data frame called `wait_times` is available and `dplyr` and `ggplot2`\n",
                "are loaded.\n",
                "\n",
                "-   Set the random seed to `334`.\n",
                "-   Generate 1000 wait times from the continuous uniform distribution that models Amir's wait time. Add this as a new column called `time` in the `wait_times` data frame.\n",
                "-   Create a histogram of the simulated wait times with 30 bins.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "wait_times <- data.frame(simulation_nb = 1:1000)\n",
                "\n",
                "# Set random seed to 334\n",
                "set.seed(334)\n",
                "\n",
                "# Set random seed to 334\n",
                "set.seed(334)\n",
                "\n",
                "# Generate 1000 wait times between 0 and 30 mins, save in time column\n",
                "wait_times %>%\n",
                "  mutate(time = runif(1000, min = 0, max = 30))\n",
                "\n",
                "# Set random seed to 334\n",
                "set.seed(334)\n",
                "\n",
                "# Generate 1000 wait times between 0 and 30 mins, save in time column\n",
                "wait_times %>%\n",
                "  mutate(time = runif(1000, min = 0, max = 30)) %>%\n",
                "  # Create a histogram of simulated times\n",
                "  ggplot(aes(time)) +\n",
                "  geom_histogram(bins = 30)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The binomial distribution\n",
                "\n",
                "### Simulating sales deals\n",
                "\n",
                "Assume that Amir usually works on 3 deals per week, and overall, he wins\n",
                "30% of deals he works on. Each deal has a binary outcome: it's either\n",
                "lost, or won, so you can model his sales deals with a binomial\n",
                "distribution. In this exercise, you'll help Amir simulate a year's worth\n",
                "of his deals so he can better understand his performance.\n",
                "\n",
                "-   Set the random seed to 10 and simulate a single deal.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Simulate a typical week of Amir's deals, or one week of 3 deals.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Simulate a year's worth of Amir's deals, or 52 weeks of 3 deals\n",
                "    each, and store in `deals`.\n",
                "-   Calculate the mean number of deals he won per week.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seed to 10\n",
                "set.seed(10)\n",
                "\n",
                "# Simulate a single deal\n",
                "rbinom(1, 1, 0.3)\n",
                "\n",
                "# Set random seed to 10\n",
                "set.seed(10)\n",
                "\n",
                "# Simulate 1 week of 3 deals\n",
                "rbinom(1, 3, 0.3)\n",
                "\n",
                "# Set random seed to 10\n",
                "set.seed(10)\n",
                "\n",
                "# Simulate 52 weeks of 3 deals\n",
                "deals <- rbinom(52, 3, 0.3)\n",
                "\n",
                "# Calculate mean deals won per week\n",
                "mean(deals)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Calculating binomial probabilities\n",
                "\n",
                "Just as in the last exercise, assume that Amir wins 30% of deals. He\n",
                "wants to get an idea of how likely he is to close a certain number of\n",
                "deals each week. In this exercise, you'll calculate what the chances are\n",
                "of him closing different numbers of deals using the binomial\n",
                "distribution.\n",
                "\n",
                "-   What's the probability that Amir closes all 3 deals in a week?\n",
                "-   What's the probability that Amir closes 1 or fewer deals in a week?\n",
                "-   What's the probability that Amir closes more than 1 deal?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Probability of closing 3 out of 3 deals\n",
                "dbinom(3, 3, 0.3)\n",
                "\n",
                "# Probability of closing <= 1 deal out of 3 deals\n",
                "pbinom(1, 3, 0.3)\n",
                "\n",
                "# Probability of closing > 1 deal out of 3 deals\n",
                "pbinom(1, 3, 0.3, lower.tail = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### How many sales will be won?\n",
                "\n",
                "Now Amir wants to know how many deals he can expect to close each week\n",
                "if his win rate changes. Luckily, you can use your binomial distribution\n",
                "knowledge to help him calculate the expected value in different\n",
                "situations. Recall from the video that the expected value of a binomial\n",
                "distribution can be calculated by \\\\n \\times p\\\\.\n",
                "\n",
                "-   Calculate the expected number of sales out of the **3** he works on\n",
                "    that Amir will win each week if he maintains his 30% win rate.\n",
                "-   Calculate the expected number of sales out of the 3 he works on that\n",
                "    he'll win if his win rate drops to 25%.\n",
                "-   Calculate the expected number of sales out of the 3 he works on that\n",
                "    he'll win if his win rate rises to 35%.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Expected number won with 30% win rate\n",
                "won_30pct <- 3 * 0.3\n",
                "won_30pct\n",
                "\n",
                "# Expected number won with 25% win rate\n",
                "won_25pct <- 3 * 0.25\n",
                "won_25pct\n",
                "\n",
                "# Expected number won with 35% win rate\n",
                "won_35pct <- 3 * 0.35\n",
                "won_35pct\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# More Distributions and the Central Limit Theorem\n",
                "\n",
                "It’s time to explore one of the most important probability distributions in statistics, normal distribution. You’ll create histograms to plot normal distributions and gain an understanding of the central limit theorem, before expanding your knowledge of statistical functions by adding the Poisson, exponential, and t-distributions to your repertoire.\n",
                "\n",
                "## The normal distribution\n",
                "\n",
                "### Distribution of Amir's sales\n",
                "\n",
                "Since each deal Amir worked on (both won and lost) was different, each\n",
                "was worth a different amount of money. These values are stored in the\n",
                "`amount` column of `amir_deals` As part of Amir's performance review,\n",
                "you want to be able to estimate the probability of him selling different\n",
                "amounts, but before you can do this, you'll need to determine what kind\n",
                "of distribution the `amount` variable follows.\n",
                "\n",
                "Both `dplyr` and `ggplot2` are loaded and `amir_deals` is available.\n",
                "\n",
                "-   Create a histogram with 10 bins to visualize the distribution of the\n",
                "    `amount`.\n",
                "\n",
                "Which probability distribution do the sales `amounts` most closely follow?\n",
                "\n",
                "\n",
                "- [ ] Uniform\n",
                "- [ ] Binomial\n",
                "- [x] Normal\n",
                "- [ ] None of the above\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Histogram of amount with 10 bins\n",
                "ggplot(amir_deals, aes(amount)) +\n",
                "  geom_histogram(bins = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Probabilities from the normal distribution\n",
                "\n",
                "Since each deal Amir worked on (both won and lost) was different, each\n",
                "was worth a different amount of money. These values are stored in the\n",
                "`amount` column of `amir_deals` and follow a normal distribution with a\n",
                "mean of 5000 dollars and a standard deviation of 2000 dollars. As part\n",
                "of his performance metrics, you want to calculate the probability of\n",
                "Amir closing a deal worth various amounts.\n",
                "\n",
                "-   What's the probability of Amir closing a deal worth less than $7500?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What's the probability of Amir closing a deal worth more than $1000?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What's the probability of Amir closing a deal worth between $3000\n",
                "    and $7000?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What amount will 75% of Amir's sales be *more than*?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Probability of deal < 7500\n",
                "pnorm(7500, mean = 5000, sd = 2000)\n",
                "\n",
                "# Probability of deal > 1000\n",
                "pnorm(1000, mean = 5000, sd = 2000, lower.tail = FALSE)\n",
                "\n",
                "# Probability of deal between 3000 and 7000\n",
                "pnorm(7000, mean = 5000, sd = 2000) - pnorm(3000, mean = 5000, sd = 2000)\n",
                "\n",
                "# Calculate amount that 75% of deals will be more than\n",
                "qnorm(0.75, mean = 5000, sd = 2000, lower.tail = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Simulating sales under new market conditions\n",
                "\n",
                "The company's financial analyst is predicting that next quarter, the\n",
                "worth of each sale will increase by 20% and the volatility, or standard\n",
                "deviation, of each sale's worth will increase by 30%. To see what Amir's\n",
                "sales might look like next quarter under these new market conditions,\n",
                "you'll simulate new sales amounts using the normal distribution and\n",
                "store these in the `new_sales` data frame, which has already been\n",
                "created for you.\n",
                "\n",
                "In addition, `dplyr` and `ggplot2` are loaded.\n",
                "\n",
                "-   Currently, Amir's average sale amount is $5000. Calculate what his\n",
                "    new average amount will be if it increases by 20% and store this in\n",
                "    `new_mean`.\n",
                "-   Amir's current standard deviation is $2000. Calculate what his new\n",
                "    standard deviation will be if it increases by 30% and store this in\n",
                "    `new_sd`.\n",
                "-   Add a new column called `amount` to the data frame `new_sales`,\n",
                "    which contains 36 simulated amounts from a normal distribution with\n",
                "    a mean of `new_mean` and a standard deviation of `new_sd`.\n",
                "-   Plot the distribution of the `new_sales` `amount`s using a histogram\n",
                "    with 10 bins.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "new_sales <- data.frame(sale_num = 1:36)\n",
                "\n",
                "# Calculate new average amount\n",
                "new_mean <- 5000 * 1.2\n",
                "\n",
                "# Calculate new standard deviation\n",
                "new_sd <- 2000 * 1.3\n",
                "\n",
                "# Simulate 36 sales\n",
                "new_sales <- new_sales %>% \n",
                "  mutate(amount = rnorm(36, mean = new_mean, sd = new_sd))\n",
                "\n",
                "# Create histogram with 10 bins\n",
                "ggplot(new_sales, aes(amount)) +\n",
                "  geom_histogram(bins = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Which market is better?\n",
                "\n",
                "The key metric that the company uses to evaluate salespeople is the\n",
                "percent of sales they make over $1000 since the time put into each sale\n",
                "is usually worth a bit more than that, so the higher this metric, the\n",
                "better the salesperson is performing.\n",
                "\n",
                "Recall that Amir's current sales amounts have a mean of $5000 and a\n",
                "standard deviation of $2000, and Amir's predicted amounts in next\n",
                "quarter's market have a mean of $6000 and a standard deviation of $2600.\n",
                "\n",
                "Based only on the metric of **percent of sales over $1000**, does Amir\n",
                "perform better in the current market or the predicted market?\n",
                "\n",
                "- [ ] Amir performs much better in the current market.\n",
                "- [ ] Amir performs much better in next quarter's predicted market.\n",
                "- [x] Amir performs about equally in both markets.\n",
                "\n",
                "## The central limit theorem\n",
                "\n",
                "### Visualizing sampling distributions\n",
                "\n",
                "On the right, try creating *sampling distributions* of different summary\n",
                "statistics from samples of different distributions. Which distribution\n",
                "does the central limit theorem **not** apply to?\n",
                "\n",
                "- [ ] Discrete uniform distribution\n",
                "- [ ] Continuous uniform distribution\n",
                "- [ ] Binomial distribution\n",
                "- [ ] All of the above\n",
                "- [x] None of the above\n",
                "\n",
                "### The CLT in action\n",
                "\n",
                "The central limit theorem states that a sampling distribution of a\n",
                "sample statistic approaches the normal distribution as you take more\n",
                "samples, no matter the original distribution being sampled from.\n",
                "\n",
                "In this exercise, you'll focus on the sample mean and see the central\n",
                "limit theorem in action while examining the `num_users` column of\n",
                "`amir_deals` more closely, which contains the number of people who\n",
                "intend to use the product Amir is selling.\n",
                "\n",
                "Both `dplyr` and `ggplot2` are loaded and `amir_deals` is available.\n",
                "\n",
                "-   Create a histogram of the `num_users` column of `amir_deals`. Use 10\n",
                "    bins.\n",
                "-   Set the seed to `104`.\n",
                "-   Take a sample of size `20` with replacement from the `num_users` column of `amir_deals`, and take the mean.\n",
                "-   Repeat this 100 times and store as `sample_means`. This will take 100 different samples and calculate the mean of each.\n",
                "-   A data frame called `samples` has been created for you with a column `mean`, which contains the values from `sample_means`. Create a histogram of the `mean` column with 10 bins.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a histogram of num_users\n",
                "ggplot(amir_deals, aes(num_users)) +\n",
                "  geom_histogram(bins = 10)\n",
                "\n",
                "# Set seed to 104\n",
                "set.seed(104)\n",
                "\n",
                "# Sample 20 num_users with replacement from amir_deals\n",
                "sample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n",
                "  # Take mean\n",
                "  mean()\n",
                "\n",
                "# Set seed to 104\n",
                "set.seed(104)\n",
                "\n",
                "# Sample 20 num_users from amir_deals and take mean\n",
                "sample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n",
                "  mean()\n",
                "\n",
                "# Repeat the above 100 times\n",
                "sample_means <- replicate(100, sample(amir_deals$num_users, size = 20, replace = TRUE) %>% mean())\n",
                "\n",
                "# Set seed to 104\n",
                "set.seed(104)\n",
                "\n",
                "# Sample 20 num_users from amir_deals and take mean\n",
                "sample(amir_deals$num_users, size = 20, replace = TRUE) %>%\n",
                "  mean()\n",
                "\n",
                "# Repeat the above 100 times\n",
                "sample_means <- replicate(100, sample(amir_deals$num_users, size = 20, replace = TRUE) %>% mean())\n",
                "\n",
                "# Create data frame for plotting\n",
                "samples <- data.frame(mean = sample_means)\n",
                "\n",
                "# Histogram of sample means\n",
                "ggplot(samples, aes(mean)) +\n",
                "  geom_histogram(bins = 10)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The mean of means\n",
                "\n",
                "You want to know what the average number of users (`num_users`) is per\n",
                "deal, but you want to know this number for the entire company so that\n",
                "you can see if Amir's deals have more or fewer users than the company's\n",
                "average deal. The problem is that over the past year, the company has\n",
                "worked on more than ten thousand deals, so it's not realistic to compile\n",
                "all the data. Instead, you'll estimate the mean by taking several random\n",
                "samples of deals, since this is much easier than collecting data from\n",
                "everyone in the company.\n",
                "\n",
                "The user data for all the company's deals is available in `all_deals`.\n",
                "\n",
                "-   Set the random seed to `321`.\n",
                "-   Take 30 samples of size 20 from `all_deals$num_users` and take the\n",
                "    mean of each sample. Store the sample means in `sample_means`.\n",
                "-   Take the mean of `sample_means`.\n",
                "-   Take the mean of the `num_users` column of `amir_deals`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "all_deals <- read.csv('all_deals.csv')\n",
                "\n",
                "# Set seed to 321\n",
                "set.seed(321)\n",
                "\n",
                "# Take 30 samples of 20 values of num_users, take mean of each sample\n",
                "sample_means <- replicate(30, sample(all_deals$num_users, 20) %>% mean())\n",
                "\n",
                "# Calculate mean of sample_means\n",
                "mean(sample_means)\n",
                "\n",
                "# Calculate mean of num_users in amir_deals\n",
                "mean(amir_deals$num_users)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Poisson distribution\n",
                "\n",
                "### Identifying lambda\n",
                "\n",
                "Now that you've learned about the Poisson distribution, you know that its shape is described by a value called lambda. In this exercise, you'll match histograms to lambda values.\n",
                "\n",
                "##### lambda = 1\n",
                "\n",
                "![highest probabilities at 0 and 1](https://bit.ly/2B7aidP)\n",
                "\n",
                "##### lambda = 4\n",
                "\n",
                "![highest probabilities at 3 and 4](https://bit.ly/2M8ZFtf)\n",
                "\n",
                "##### lambda = 8\n",
                "\n",
                "![highest probabilities at 7 and 8](https://bit.ly/2TMUvYe)\n",
                "\n",
                "\n",
                "### Tracking lead responses\n",
                "\n",
                "Your company uses sales software to keep track of new sales leads. It\n",
                "organizes them into a queue so that anyone can follow up on one when\n",
                "they have a bit of free time. Since the number of lead responses is a\n",
                "countable outcome over a period of time, this scenario corresponds to a\n",
                "Poisson distribution. On average, Amir responds to 4 leads each day. In\n",
                "this exercise, you'll calculate probabilities of Amir responding to\n",
                "different numbers of leads.\n",
                "\n",
                "-   What's the probability that Amir responds to 5 leads in a day, given\n",
                "    that he responds to an average of 4?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Amir's coworker responds to an average of 5.5 leads per day. What is\n",
                "    the probability that she answers 5 leads in a day?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What's the probability that Amir responds to 2 or fewer leads in a\n",
                "    day?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What's the probability that Amir responds to more than 10 leads in a\n",
                "    day?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Probability of 5 responses\n",
                "dpois(5, lambda = 4)\n",
                "\n",
                "# Probability of 5 responses from coworker\n",
                "dpois(5, lambda = 5.5)\n",
                "\n",
                "# Probability of 2 or fewer responses\n",
                "ppois(2, lambda = 4)\n",
                "\n",
                "# Probability of > 10 responses\n",
                "ppois(10, lambda = 4, lower.tail = FALSE)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## More probability distributions\n",
                "\n",
                "### Too many distributions\n",
                "\n",
                "By this point, you've learned about so many different probability distributions that it can be difficult to remember which is which. In this exercise, you'll practice distinguishing between distributions and identifying the distribution that best matches different scenarios.\n",
                "\n",
                "-   Match each situation to the distribution that best models it.\n",
                "\n",
                "##### Poisson\n",
                "\n",
                "- Number of customers that enter a store each hour\n",
                "- Number of products sold each week\n",
                "\n",
                "##### Exponential\n",
                "\n",
                "- Amount of time until someone pays off their loan\n",
                "- Amount of time until the next customer makes a purchase\n",
                "\n",
                "##### Binomial\n",
                "\n",
                "- Number of people from a group of 30 that pass their driving test\n",
                "\n",
                "### Modeling time between leads\n",
                "\n",
                "To further evaluate Amir's performance, you want to know how much time\n",
                "it takes him to respond to a lead after he opens it. On average, it\n",
                "takes 2.5 hours for him to respond. In this exercise, you'll calculate\n",
                "probabilities of different amounts of time passing between Amir\n",
                "receiving a lead and sending a response.\n",
                "\n",
                "-   What's the probability it takes Amir less than an hour to respond to\n",
                "    a lead?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What's the probability it takes Amir more than 4 hours to respond to\n",
                "    a lead?\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   What's the probability it takes Amir 3-4 hours to respond to a lead?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Probability response takes < 1 hour\n",
                "pexp(1, rate = 1/2.5)\n",
                "\n",
                "# Probability response takes > 4 hours\n",
                "pexp(4, rate = 1/2.5, lower.tail = FALSE)\n",
                "\n",
                "# Probability response takes 3-4 hours\n",
                "pexp(4, rate = 1/2.5) - pexp(3, rate = 1/2.5)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The t-distribution\n",
                "\n",
                "Which statement is **not** true regarding the t-distribution?\n",
                "\n",
                "- [ ] The t-distribution has thicker tails than the normal distribution.\n",
                "- [ ] A t-distribution with high degrees of freedom resembles the normal distribution.\n",
                "- [ ] The number of degrees of freedom affects the distribution's variance.\n",
                "- [x] The t-distribution is skewed.\n",
                "\n",
                "# Correlation and Experimental Design\n",
                "\n",
                "In this chapter, you'll learn how to quantify the strength of a linear relationship between two variables, and explore how confounding variables can affect the relationship between two other variables. You'll also see how a study’s design can influence its results, change how the data should be analyzed, and potentially affect the reliability of your conclusions.\n",
                "\n",
                "## Correlation\n",
                "\n",
                "### Guess the correlation\n",
                "\n",
                "On the right, use the scatterplot to estimate what the correlation is\n",
                "between the variables `x` and `y`. Once you've guessed it correctly, use\n",
                "the **New Plot** button to try out a few more scatterplots. When you're\n",
                "ready, answer the question below to continue to the next exercise.\n",
                "\n",
                "Which of the following statements is NOT true about correlation?\n",
                "\n",
                "- [ ] If the correlation between \\`x\\` and \\`y\\` has a high magnitude, the\n",
                "    data points will be clustered closely around a line.\n",
                "- [ ] Correlation can be written as \\*r\\*.\n",
                "- [ ] If \\`x\\` and \\`y\\` are negatively correlated, values of \\`y\\`\n",
                "    decrease as values of \\`x\\` increase.\n",
                "- [x] Correlation cannot be 0.\n",
                "\n",
                "\n",
                "### Relationships between variables\n",
                "\n",
                "In this chapter, you'll be working with a dataset `world_happiness`\n",
                "containing results from the [2019 World Happiness\n",
                "Report](https://worldhappiness.report/ed/2019/). The report scores\n",
                "various countries based on how happy people in that country are. It also\n",
                "ranks each country on various societal aspects such as social support,\n",
                "freedom, corruption, and others. The dataset also includes the GDP per\n",
                "capita and life expectancy for each country.\n",
                "\n",
                "In this exercise, you'll examine the relationship between a country's\n",
                "life expectancy (`life_exp`) and happiness score (`happiness_score`)\n",
                "both visually and quantitatively. Both `dplyr` and `ggplot2` are loaded\n",
                "and `world_happiness` is available.\n",
                "\n",
                "-   Create a scatterplot of `happiness_score` vs. `life_exp` using\n",
                "    `ggplot2`.\n",
                "-   Add a linear trendline to the scatterplot, setting `se` to `FALSE`.\n",
                "\n",
                "Based on the scatterplot, which is most likely the correlation between `life_exp` and `happiness_score`?\n",
                "\n",
                "- [ ] 0.3\n",
                "- [ ] -0.3\n",
                "- [x] 0.8\n",
                "- [ ] -0.8\n",
                "\n",
                "-   Calculate the correlation between `life_exp` and `happiness_score`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# edited/added\n",
                "world_happiness <- readRDS('world_happiness_sugar.rds')\n",
                "\n",
                "# Create a scatterplot of happiness_score vs. life_exp\n",
                "ggplot(world_happiness, aes(life_exp, happiness_score)) +\n",
                "  geom_point()\n",
                "\n",
                "# Add a linear trendline to scatterplot\n",
                "ggplot(world_happiness, aes(life_exp, happiness_score)) +\n",
                "  geom_point() +\n",
                "  geom_smooth(method = \"lm\", se = FALSE)\n",
                "\n",
                "# Add a linear trendline to scatterplot\n",
                "ggplot(world_happiness, aes(life_exp, happiness_score)) +\n",
                "  geom_point() +\n",
                "  geom_smooth(method = \"lm\", se = FALSE)\n",
                "\n",
                "# Correlation between life_exp and happiness_score\n",
                "cor(world_happiness$life_exp, world_happiness$happiness_score)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Correlation caveats\n",
                "\n",
                "### What can't correlation measure?\n",
                "\n",
                "While the correlation coefficient is a convenient way to quantify the\n",
                "strength of a relationship between two variables, it's far from perfect.\n",
                "In this exercise, you'll explore one of the caveats of the correlation\n",
                "coefficient by examining the relationship between a country's GDP per\n",
                "capita (`gdp_per_cap`) and happiness score.\n",
                "\n",
                "Both `dplyr` and `ggplot2` are loaded and `world_happiness` is\n",
                "available.\n",
                "\n",
                "-   Create a scatterplot showing the relationship between `gdp_per_cap`\n",
                "    (on the x-axis) and `life_exp` (on the y-axis).\n",
                "-   Calculate the correlation between `gdp_per_cap` and `life_exp`.\n",
                "\n",
                "The correlation between GDP per capita and life expectancy is 0.7. Why\n",
                "is correlation ***not*** the best way to measure the relationship\n",
                "between the two variables?\n",
                "\n",
                "- [ ] Correlation measures how one variable affects another.\n",
                "- [x] Correlation only measures linear relationships.\n",
                "- [ ] Correlation cannot properly measure relationships between numeric variables.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scatterplot of gdp_per_cap and life_exp\n",
                "ggplot(world_happiness, aes(gdp_per_cap, life_exp)) +\n",
                "  geom_point()\n",
                "\n",
                "# Scatterplot of gdp_per_cap and life_exp\n",
                "ggplot(world_happiness, aes(gdp_per_cap, life_exp)) +\n",
                "  geom_point()\n",
                "\n",
                "# Correlation between gdp_per_cap and life_exp\n",
                "cor(world_happiness$gdp_per_cap, world_happiness$life_exp)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Transforming variables\n",
                "\n",
                "When variables have skewed distributions, they often require a\n",
                "transformation in order to form a linear relationship with another\n",
                "variable so that correlation can be computed. In this exercise, you'll\n",
                "perform a transformation yourself.\n",
                "\n",
                "Both `dplyr` and `ggplot2` are loaded and `world_happiness` is\n",
                "available.\n",
                "\n",
                "-   Create a scatterplot of `happiness_score` versus `gdp_per_cap`.\n",
                "-   Calculate the correlation between `happiness_score` and\n",
                "    `gdp_per_cap`.\n",
                "\n",
                "<!-- -->\n",
                "\n",
                "-   Add a new column to `world_happiness` called `log_gdp_per_cap` that\n",
                "    contains the log of `gdp_per_cap`.\n",
                "-   Create a scatterplot of `happiness_score` versus `log_gdp_per_cap`.\n",
                "-   Calculate the correlation between `happiness_score` and\n",
                "    `log_gdp_per_cap`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scatterplot of happiness_score vs. gdp_per_cap\n",
                "ggplot(world_happiness, aes(gdp_per_cap, happiness_score)) +\n",
                "  geom_point()\n",
                "\n",
                "# Calculate correlation\n",
                "cor(world_happiness$gdp_per_cap, world_happiness$happiness_score)\n",
                "\n",
                "# Create log_gdp_per_cap column\n",
                "world_happiness <- world_happiness %>%\n",
                "  mutate(log_gdp_per_cap = log(gdp_per_cap))\n",
                "\n",
                "# Scatterplot of happiness_score vs. log_gdp_per_cap\n",
                "ggplot(world_happiness, aes(log_gdp_per_cap, happiness_score)) +\n",
                "  geom_point()\n",
                "\n",
                "# Calculate correlation\n",
                "cor(world_happiness$log_gdp_per_cap, world_happiness$happiness_score)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Does sugar improve happiness?\n",
                "\n",
                "A new column has been added to `world_happiness` called\n",
                "`grams_sugar_per_day`, which contains the average amount of sugar eaten\n",
                "per person per day in each country. In this exercise, you'll examine the\n",
                "effect of a country's average sugar consumption on its happiness score.\n",
                "\n",
                "Both `dplyr` and `ggplot2` are loaded and `world_happiness` is\n",
                "available.\n",
                "\n",
                "-   Create a scatterplot showing the relationship between\n",
                "    `grams_sugar_per_day` (on the x-axis) and `happiness_score` (on the\n",
                "    y-axis).\n",
                "-   Calculate the correlation between `grams_sugar_per_day` and\n",
                "    `happiness_score`.\n",
                "\n",
                "Based on this data, which statement about sugar consumption and happiness scores is true?\n",
                " \n",
                "- [ ] Increased sugar consumption leads to a higher happiness score.\n",
                "- [ ] Lower sugar consumption results in a lower happiness score\n",
                "- [x] Increased sugar consumption is associated with a higher happiness score.\n",
                "- [ ] Sugar consumption is not related to happiness.\n",
                " \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scatterplot of grams_sugar_per_day and happiness_score\n",
                "ggplot(world_happiness, aes(grams_sugar_per_day, happiness_score)) +\n",
                "  geom_point()\n",
                "\n",
                "# Correlation between grams_sugar_per_day and happiness_score\n",
                "cor(world_happiness$grams_sugar_per_day, world_happiness$happiness_score)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Confounders\n",
                "\n",
                "A study is investigating the relationship between neighborhood residence and lung capacity. Researchers measure the lung capacity of thirty people from neighborhood A, which is located near a highway, and thirty people from neighborhood B, which is not near a highway. Both groups have similar smoking habits and a similar gender breakdown.\n",
                "\n",
                "Which of the following could be a confounder in this study?\n",
                "\n",
                "- [ ] Lung capacity\n",
                "- [ ] Neighborhood\n",
                "- [x] Air pollution\n",
                "- [ ] Smoking status\n",
                "- [ ] Gender\n",
                "\n",
                "## Design of experiments\n",
                "\n",
                "### Study types\n",
                "\n",
                "While controlled experiments are ideal, many situations and research questions are not conducive to a controlled experiment. In a controlled experiment, causation can likely be inferred if the control and test groups have similar characteristics and don't have any systematic difference between them. On the other hand, causation cannot usually be inferred from observational studies, whose results are often misinterpreted as a result.\n",
                "\n",
                "In this exercise, you'll practice distinguishing controlled experiments from observational studies.\n",
                "\n",
                "-   Determine if each study is a controlled experiment or observational study.\n",
                "\n",
                "##### Controlled experiment\n",
                "\n",
                "- Subjects are randomly assigned to a diet and weight loss is compared.\n",
                "- Asthma symptoms are compared between children randomly assigned to\n",
                "receive professional home pest management services or pest management\n",
                "education.\n",
                "- Purchasing rates are compared between users of an e-commerce site who\n",
                "are randomly directed to a new version of the home page or an old\n",
                "version.\n",
                "\n",
                "##### Observational study\n",
                "\n",
                "- Prevalence of heart disease is compared between veterans with PTSD and\n",
                "veterans without PTSD.\n",
                "- A week ago, the home page of an e-commerce site was updated. Purchasing\n",
                "rates are compared between users who saw the old and new home page\n",
                "versions.\n",
                "\n",
                "### Longitudinal vs. cross-sectional studies\n",
                "\n",
                "A company manufactures thermometers, and they want to study the relationship between a thermometer's age and its accuracy. To do this, they take a sample of 100 different thermometers of different ages and test how accurate they are. Is this data longitudinal or cross-sectional?\n",
                "\n",
                "- [ ] Longitudinal\n",
                "- [x] Cross-sectional\n",
                "- [ ] Both\n",
                "- [ ] Neither\n",
                "\n",
                "## Congratulations!\n",
                "\n",
                "### Congratulations!\n",
                "\n",
                "Congratulations on completing the course! You now have foundational statistics skills that you can use in your analyses and build upon further.\n",
                "\n",
                "### Overview\n",
                "\n",
                "In the first chapter of the course, you learned about what statistics can do, as well as summary statistics to measure the center and spread of a distribution. In the second chapter, you learned how to measure chance and how to use and interpret probability distributions. You also learned about the binomial distribution. In chapter three, you learned about the normal distribution and the central limit theorem, one of the most important ideas in statistics. You also saw how the Poisson distribution can be used to model countable outcomes. In the final chapter, you saw how to quantify relationships between two variables using correlation. You also learned about controlled experiments and observational studies and the conclusions that can and cannot be drawn from them.\n",
                "\n",
                "### Build on your skills\n",
                "\n",
                "There's still much more that you can do with statistics and much more to learn. Your new skills will set you up for success in this course on the foundations of regression.\n",
                "\n",
                "### Congratulations!\n",
                "\n",
                "Thanks for accompanying me on this statistical journey. Congratulations again!\n"
            ]
        }
    ],
    "metadata": {
        "anaconda-cloud": "",
        "kernelspec": {
            "display_name": "R",
            "langauge": "R",
            "name": "ir"
        },
        "language_info": {
            "codemirror_mode": "r",
            "file_extension": ".r",
            "mimetype": "text/x-r-source",
            "name": "R",
            "pygments_lexer": "r",
            "version": "3.4.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
